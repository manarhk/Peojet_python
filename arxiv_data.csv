Title,Author,URL,Published,Summary,ID,Category
Egg-smol Python: A Pythonic Library for E-graphs,Saul Shanabrook,"[{'@href': 'http://arxiv.org/abs/2305.04311v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.04311v1', '@rel': 'related', '@type': 'application/pdf'}]",2023-05-07T15:35:17Z,"E-graphs have emerged as a versatile data structure with applications in
synthesis, optimization, and verification through techniques such as equality
saturation. This paper introduces Python bindings for the experimental egg-smol
library, which aims to bring the benefits of e-graphs to the Python ecosystem.
The bindings offer a high-level, Pythonic API providing an accessible and
familiar interface for Python users. By integrating e-graph techniques with
Python, we hope to enable collaboration and innovation across various domains
in the scientific computing and machine learning communities. We discuss the
advantages of using Python bindings for both Python and existing egg-smol
users, as well as possible future directions for development.",http://arxiv.org/abs/2305.04311v1,
"Making Python Code Idiomatic by Automatic Refactoring Non-Idiomatic
  Python Code with Pythonic Idioms",Zejun Zhang,"[{'@href': 'http://arxiv.org/abs/2207.05613v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2207.05613v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-07-12T15:30:46Z,"Compared to other programming languages (e.g., Java), Python has more idioms
to make Python code concise and efficient. Although pythonic idioms are well
accepted in the Python community, Python programmers are often faced with many
challenges in using them, for example, being unaware of certain pythonic idioms
or do not know how to use them properly. Based on an analysis of 7,638 Python
repositories on GitHub, we find that non-idiomatic Python code that can be
implemented with pythonic idioms occurs frequently and widely. Unfortunately,
there is no tool for automatically refactoring such non-idiomatic code into
idiomatic code. In this paper, we design and implement an automatic refactoring
tool to make Python code idiomatic. We identify nine pythonic idioms by
systematically contrasting the abstract syntax grammar of Python and Java. Then
we define the syntactic patterns for detecting non-idiomatic code for each
pythonic idiom. Finally, we devise atomic AST-rewriting operations and
refactoring steps to refactor non-idiomatic code into idiomatic code. We test
and review over 4,115 refactorings applied to 1,065 Python projects from
GitHub, and submit 90 pull requests for the 90 randomly sampled refactorings to
84 projects. These evaluations confirm the high-accuracy, practicality and
usefulness of our refactoring tool on real-world Python code. Our refactoring
tool can be accessed at 47.242.131.128:5000.",http://arxiv.org/abs/2207.05613v1,
Modern Python at the Large Synoptic Survey Telescope,Tim Jenness,"[{'@href': 'http://arxiv.org/abs/1712.00461v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1712.00461v1', '@rel': 'related', '@type': 'application/pdf'}]",2017-12-01T19:04:46Z,"The LSST software systems make extensive use of Python, with almost all of it
initially being developed solely in Python 2. Since LSST will be commissioned
when Python 2 is end-of-lifed it is critical that we have all our code support
Python 3 before commissioning begins. Over the past year we have made
significant progress in migrating the bulk of the code from the Data Management
system onto Python 3. This paper presents our migration methodology, and the
current status of the port, with our eventual aim to be running completely on
Python 3 by early 2018. We also discuss recent modernizations to our Python
codebase.",http://arxiv.org/abs/1712.00461v1,
Python GUI Scripting Interface for Running Atomic Physics Applications,Amani Tahat,"[{'@href': 'http://arxiv.org/abs/1106.0868v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1106.0868v1', '@rel': 'related', '@type': 'application/pdf'}]",2011-06-05T01:11:08Z,"We create a Python GUI scripting interface working under Windows in addition
to (UNIX/Linux). The GUI has been built around the Python open-source
programming language. We use the Python's GUI library that so called Python
Mega Widgets (PMW) and based on Tkinter Python module
(http://www.freenetpages.co.uk/hp/alan.gauld/tutgui.htm). The new GUI was
motivated primarily by the desire of more updated operations, more flexibility
incorporating future and current improvements in producing atomic data.
Furthermore it will be useful for a variety of applications of atomic physics,
plasma physics and astrophysics and will help in calculating various atomic
properties.",http://arxiv.org/abs/1106.0868v1,
Towards Memory Safe Python Enclave for Security Sensitive Computation,Huibo Wang,"[{'@href': 'http://arxiv.org/abs/2005.05996v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2005.05996v1', '@rel': 'related', '@type': 'application/pdf'}]",2020-05-12T18:19:08Z,"Intel SGX Guard eXtensions (SGX), a hardware-supported trusted execution
environment (TEE), is designed to protect security-sensitive applications.
However, since enclave applications are developed with memory unsafe languages
such as C/C++, traditional memory corruption is not eliminated in SGX. Rust-SGX
is the first toolkit providing enclave developers with a memory-language.
However, Rust is considered a Systems language and has become the right choice
for concurrent applications and web browsers. Many application domains such as
Big Data, Machine Learning, Robotics, Computer Vision are more commonly
developed in the python programming language. Therefore, Python application
developers cannot benefit from secure enclaves like Intel SGX and rust-SGX. To
fill this gap, we propose Python-SGX, which is a memory-safe SGX SDK providing
enclave developers a memory-safe Python development environment. The key idea
is to enable memory-safe Python language in SGX by solving the following key
challenges: (1) defining a memory-safe Python interpreter (2)replacing unsafe
elements of Python interpreter with safe ones,(3) achieving comparable
performance to non-enclave Python applications, and (4) not introducing any
unsafe new code or libraries into SGX. We propose to build Python-SGX with
PyPy, a Python interpreter written by RPython, which is a subset of Python, and
tame unsafe parts in PyPy by formal verification, security hardening, and
memory safe language. We have implemented python-SGX and tested it with a
series of benchmarks programs. Our evaluation results show that Python-SGX does
not cause significant overhead.",http://arxiv.org/abs/2005.05996v1,
Porting the LSST Data Management Pipeline Software to Python 3,Tim Jenness,"[{'@href': 'http://arxiv.org/abs/1611.00751v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1611.00751v1', '@rel': 'related', '@type': 'application/pdf'}]",2016-11-02T19:48:34Z,"The LSST data management science pipelines software consists of more than
100,000 lines of Python 2 code. LSST operations will begin after support for
Python 2 has been dropped by the Python community in 2020, and we must
therefore plan to migrate the codebase to Python 3. During the transition
period we must also support our community of active Python 2 users and this
complicates the porting significantly. We have decided to use the Python future
package as the basis for our port to enable support for Python 2 and Python 3
simultaneously, whilst developing with a mindset more suited to Python 3. In
this paper we report on the current status of the port and the difficulties
that have been encountered.",http://arxiv.org/abs/1611.00751v1,
"A general approach for running Python codes in OpenFOAM using an
  embedded pybind11 Python interpreter",Simon Rodriguez,"[{'@href': 'http://arxiv.org/abs/2203.16394v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2203.16394v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-03-30T15:25:03Z,"As the overlap between traditional computational mechanics and machine
learning grows, there is an increasing demand for straight-forward approaches
to interface Python-based procedures with C++-based OpenFOAM. This article
introduces one such general methodology, allowing the execution of Python code
directly within an OpenFOAM solver without the need for Python code
translation. The proposed approach is based on the lightweight library
pybind11, where OpenFOAM data is transferred to an embedded Python interpreter
for manipulation, and results are returned as needed. Following a review of
related approaches, the article describes the approach, with a particular focus
on data transfer between Python and OpenFOAM, executing Python scripts and
functions, and practical details about the implementation in OpenFOAM. Three
complementary test cases are presented to highlight the functionality and
demonstrate the effect of different data transfer approaches: a Python-based
velocity profile boundary condition; a Python-based solver for prototyping; and
a machine learning mechanical constitutive law class for solids4foam which
performs field calculations.",http://arxiv.org/abs/2203.16394v1,
Python for education: the exact cover problem,Andrzej Kapanowski,"[{'@href': 'http://arxiv.org/abs/1010.5890v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1010.5890v1', '@rel': 'related', '@type': 'application/pdf'}]",2010-10-28T08:53:26Z,"Python implementation of Algorithm X by Knuth is presented. Algorithm X finds
all solutions to the exact cover problem. The exemplary results for
pentominoes, Latin squares and Sudoku are given.",http://arxiv.org/abs/1010.5890v1,
"Teddy: Automatic Recommendation of Pythonic Idiom Usage For Pull-Based
  Software Projects",Purit Phan-udom,"[{'@href': 'http://arxiv.org/abs/2009.03302v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2009.03302v1', '@rel': 'related', '@type': 'application/pdf'}]",2020-09-05T12:54:57Z,"Pythonic code is idiomatic code that follows guiding principles and practices
within the Python community. Offering performance and readability benefits,
Pythonic code is claimed to be widely adopted by experienced Python developers,
but can be a learning curve to novice programmers. To aid with Pythonic
learning, we create an automated tool, called Teddy, that can help checking the
Pythonic idiom usage. The tool offers a prevention mode with Just-In-Time
analysis to recommend the use of Pythonic idiom during code review and a
detection mode with historical analysis to run a thorough scan of idiomatic and
non-idiomatic code. In this paper, we first describe our tool and an evaluation
of its performance. Furthermore, we present a case study that demonstrates how
to use Teddy in a real-life scenario on an Open Source project. An evaluation
shows that Teddy has high precision for detecting Pythonic idiom and
non-Pythonic code. Using interactive visualizations, we demonstrate how novice
programmers can navigate and identify Pythonic idiom and non-Pythonic code in
their projects. Our video demo with the full interactive visualizations is
available at https://youtu.be/vOCQReSvBxA.",http://arxiv.org/abs/2009.03302v1,
Machine Learning using Stata/Python,Giovanni Cerulli,"[{'@href': 'http://arxiv.org/abs/2103.03122v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2103.03122v1', '@rel': 'related', '@type': 'application/pdf'}]",2021-03-03T10:31:44Z,"We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting
popular Machine Learning (ML) methods both in regression and classification
settings. Using the recent Stata/Python integration platform (sfi) of Stata 16,
these commands provide hyper-parameters' optimal tuning via K-fold
cross-validation using greed search. More specifically, they make use of the
Python Scikit-learn API to carry out both cross-validation and outcome/label
prediction.",http://arxiv.org/abs/2103.03122v1,
Using Python for Model Inference in Deep Learning,Zachary DeVito,"[{'@href': 'http://arxiv.org/abs/2104.00254v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2104.00254v1', '@rel': 'related', '@type': 'application/pdf'}]",2021-04-01T04:48:52Z,"Python has become the de-facto language for training deep neural networks,
coupling a large suite of scientific computing libraries with efficient
libraries for tensor computation such as PyTorch or TensorFlow. However, when
models are used for inference they are typically extracted from Python as
TensorFlow graphs or TorchScript programs in order to meet performance and
packaging constraints. The extraction process can be time consuming, impeding
fast prototyping. We show how it is possible to meet these performance and
packaging constraints while performing inference in Python. In particular, we
present a way of using multiple Python interpreters within a single process to
achieve scalable inference and describe a new container format for models that
contains both native Python code and data. This approach simplifies the model
deployment story by eliminating the model extraction step, and makes it easier
to integrate existing performance-enhancing Python libraries. We evaluate our
design on a suite of popular PyTorch models on Github, showing how they can be
packaged in our inference format, and comparing their performance to
TorchScript. For larger models, our packaged Python models perform the same as
TorchScript, and for smaller models where there is some Python overhead, our
multi-interpreter approach ensures inference is still scalable.",http://arxiv.org/abs/2104.00254v1,
Python Type Hints are Turing Complete,Ori Roth,"[{'@href': 'http://arxiv.org/abs/2208.14755v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2208.14755v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-08-31T10:11:42Z,"Grigore showed that Java generics are Turing complete by describing a
reduction from Turing machines to Java subtyping. We apply Grigore's algorithm
to Python type hints and deduce that they are Turing complete. In addition, we
present an alternative reduction in which the Turing machines are simulated in
real time, resulting in significantly lower compilation times. Our work is
accompanied by a Python implementation of both reductions that compiles Turing
machines into Python subtyping machines.",http://arxiv.org/abs/2208.14755v1,
"OMB-Py: Python Micro-Benchmarks for Evaluating Performance of MPI
  Libraries on HPC Systems",Nawras Alnaasan,"[{'@href': 'http://arxiv.org/abs/2110.10659v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2110.10659v2', '@rel': 'related', '@type': 'application/pdf'}]",2021-10-20T16:59:14Z,"Python has become a dominant programming language for emerging areas like
Machine Learning (ML), Deep Learning (DL), and Data Science (DS). An attractive
feature of Python is that it provides easy-to-use programming interface while
allowing library developers to enhance performance of their applications by
harnessing the computing power offered by High Performance Computing (HPC)
platforms. Efficient communication is key to scaling applications on parallel
systems, which is typically enabled by the Message Passing Interface (MPI)
standard and compliant libraries on HPC hardware. mpi4py is a Python-based
communication library that provides an MPI-like interface for Python
applications allowing application developers to utilize parallel processing
elements including GPUs. However, there is currently no benchmark suite to
evaluate communication performance of mpi4py -- and Python MPI codes in general
-- on modern HPC systems. In order to bridge this gap, we propose OMB-Py --
Python extensions to the open-source OSU Micro-Benchmark (OMB) suite -- aimed
to evaluate communication performance of MPI-based parallel applications in
Python. To the best of our knowledge, OMB-Py is the first communication
benchmark suite for parallel Python applications. OMB-Py consists of a variety
of point-to-point and collective communication benchmark tests that are
implemented for a range of popular Python libraries including NumPy, CuPy,
Numba, and PyCUDA. Our evaluation reveals that mpi4py introduces a small
overhead when compared to native MPI libraries. We plan to publicly release
OMB-Py to benefit the Python HPC community.",http://arxiv.org/abs/2110.10659v2,
"Does Coding in Pythonic Zen Peak Performance? Preliminary Experiments of
  Nine Pythonic Idioms at Scale",Pattara Leelaprute,"[{'@href': 'http://arxiv.org/abs/2203.14484v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2203.14484v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-03-28T04:05:54Z,"In the field of data science, and for academics in general, the Python
programming language is a popular choice, mainly because of its libraries for
storing, manipulating, and gaining insight from data. Evidence includes the
versatile set of machine learning, data visualization, and manipulation
packages used for the ever-growing size of available data. The Zen of Python is
a set of guiding design principles that developers use to write acceptable and
elegant Python code. Most principles revolve around simplicity. However, as the
need to compute large amounts of data, performance has become a necessity for
the Python programmer. The new idea in this paper is to confirm whether writing
the Pythonic way peaks performance at scale. As a starting point, we conduct a
set of preliminary experiments to evaluate nine Pythonic code examples by
comparing the performance of both Pythonic and Non-Pythonic code snippets. Our
results reveal that writing in Pythonic idioms may save memory and time. We
show that incorporating list comprehension, generator expression, zip, and
itertools.zip_longest idioms can save up to 7,000 MB and up to 32.25 seconds.
The results open more questions on how they could be utilized in a real-world
setting. The replication package includes all scripts, and the results are
available at https://doi.org/10.5281/zenodo.5712349",http://arxiv.org/abs/2203.14484v1,
Pydelay - a python tool for solving delay differential equations,V. Flunkert,"[{'@href': 'http://arxiv.org/abs/0911.1633v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/0911.1633v1', '@rel': 'related', '@type': 'application/pdf'}]",2009-11-09T11:00:43Z,"pydelay is a python library which translates a system of delay differential
equations into C-code and simulates the code using scipy weave.",http://arxiv.org/abs/0911.1633v1,
Performance of Python runtimes on a non-numeric scientific code,Riccardo Murri,"[{'@href': 'http://arxiv.org/abs/1404.6388v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1404.6388v2', '@rel': 'related', '@type': 'application/pdf'}]",2014-04-25T10:55:48Z,"The Python library FatGHol FatGHoL used in Murri2012 to reckon the rational
homology of the moduli space of Riemann surfaces is an example of a non-numeric
scientific code: most of the processing it does is generating graphs
(represented by complex Python objects) and computing their isomorphisms (a
triple of Python lists; again a nested data structure). These operations are
repeated many times over: for example, the spaces and are triangulated by
4'583'322 and 747'664 graphs, respectively. This is an opportunity for every
Python runtime to prove its strength in optimization. The purpose of this
experiment was to assess the maturity of alternative Python runtimes, in terms
of: compatibility with the language as implemented in CPython 2.7, and
performance speedup. This paper compares the results and experiences from
running FatGHol with different Python runtimes: CPython 2.7.5, PyPy 2.1, Cython
0.19, Numba 0.11, Nuitka 0.4.4 and Falcon.",http://arxiv.org/abs/1404.6388v2,
"A Python Extension for the Massively Parallel Multiphysics Simulation
  Framework waLBerla",Martin Bauer,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1080/17445760.2015.1118478', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/1511.07261v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1511.07261v1', '@rel': 'related', '@type': 'application/pdf'}]",2015-11-23T15:06:47Z,"We present a Python extension to the massively parallel HPC simulation
toolkit waLBerla. waLBerla is a framework for stencil based algorithms
operating on block-structured grids, with the main application field being
fluid simulations in complex geometries using the lattice Boltzmann method.
Careful performance engineering results in excellent node performance and good
scalability to over 400,000 cores. To increase the usability and flexibility of
the framework, a Python interface was developed. Python extensions are used at
all stages of the simulation pipeline: They simplify and automate scenario
setup, evaluation, and plotting. We show how our Python interface outperforms
the existing text-file-based configuration mechanism, providing features like
automatic nondimensionalization of physical quantities and handling of complex
parameter dependencies. Furthermore, Python is used to process and evaluate
results while the simulation is running, leading to smaller output files and
the possibility to adjust parameters dependent on the current simulation state.
C++ data structures are exported such that a seamless interfacing to other
numerical Python libraries is possible. The expressive power of Python and the
performance of C++ make development of efficient code with low time effort
possible.",http://arxiv.org/abs/1511.07261v1,
Generating Python Code From Object-Z Specifications,A. F. Al Azzawi,"[{'@href': 'http://arxiv.org/abs/1802.06224v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1802.06224v1', '@rel': 'related', '@type': 'application/pdf'}]",2018-02-17T11:41:24Z,"Object-Z is an object-oriented specification language which extends the Z
language with classes, objects, inheritance and polymorphism that can be used
to represent the specification of a complex system as collections of objects.
There are a number of existing works that mapped Object-Z to C++ and Java
programming languages. Since Python and Object-Z share many similarities, both
are object-oriented paradigm, support set theory and predicate calculus
moreover, Python is a functional programming language which is naturally closer
to formal specifications, we propose a mapping from Object-Z specifications to
Python code that covers some Object-Z constructs and express its specifications
in Python to validate these specifications. The validations are used in the
mapping covered preconditions, post-conditions, and invariants that are built
using lambda function and Python's decorator. This work has found Python is an
excellent language for developing libraries to map Object-Z specifications to
Python.",http://arxiv.org/abs/1802.06224v1,
Building a scalable python distribution for HEP data analysis,David Lange,"[{'@href': 'http://arxiv.org/abs/1804.08939v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1804.08939v1', '@rel': 'related', '@type': 'application/pdf'}]",2018-04-24T10:07:02Z,"There are numerous approaches to building analysis applications across the
high-energy physics community. Among them are Python-based, or at least
Python-driven, analysis workflows. We aim to ease the adoption of a
Python-based analysis toolkit by making it easier for non-expert users to gain
access to Python tools for scientific analysis. Experimental software
distributions and individual user analysis have quite different requirements.
Distributions tend to worry most about stability, usability and
reproducibility, while the users usually strive to be fast and nimble. We
discuss how we built and now maintain a python distribution for analysis while
satisfying requirements both a large software distribution (in our case, that
of CMSSW) and user, or laptop, level analysis. We pursued the integration of
tools used by the broader data science community as well as HEP developed
(e.g., histogrammar, root_numpy) Python packages. We discuss concepts we
investigated for package integration and testing, as well as issues we
encountered through this process. Distribution and platform support are
important topics. We discuss our approach and progress towards a sustainable
infrastructure for supporting this Python stack for the CMS user community and
for the broader HEP user community.",http://arxiv.org/abs/1804.08939v1,
The Dune Python Module,Andreas Dedner,"[{'@href': 'http://arxiv.org/abs/1807.05252v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1807.05252v1', '@rel': 'related', '@type': 'application/pdf'}]",2018-07-13T19:17:48Z,"In this paper we present the new Dune-Python module which provides Python
bindings for the Dune core, which is a C++ environment for solving partial
differential equations. The aim of this new module is to firstly provide the
general infrastructure for exporting realizations of statically polymorphic
interfaces based on just-in-time compilation and secondly to provide bindings
for the central interfaces of the dune core modules. In the first release we
focus on the grid interface. Our aim is to only introduce a thin layer when
passing objects into Python which can be removed when the object is passed back
into a C++ algorithm. Thus no efficiency is lost and little additional code
maintenance cost is incurred. To make the transition for Dune users to the
Python environment straightforward the Python classes provide a very similar
interface to their C++ counterparts. In addition, vectorized versions of many
interfaces allow for more efficient code on the Python side. The infrastructure
for exporting these interfaces and the resulting bindings for a Dune grid are
explained in detail in this paper for both experienced Dune users and others
interested in a flexible Python environment for implementing grid based schemes
for solving partial differential equations.",http://arxiv.org/abs/1807.05252v1,
How fast can we make interpreted Python?,Russell Power,"[{'@href': 'http://arxiv.org/abs/1306.6047v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1306.6047v2', '@rel': 'related', '@type': 'application/pdf'}]",2013-06-25T17:57:00Z,"Python is a popular dynamic language with a large part of its appeal coming
from powerful libraries and extension modules. These augment the language and
make it a productive environment for a wide variety of tasks, ranging from web
development (Django) to numerical analysis (NumPy). Unfortunately, Python's
performance is quite poor when compared to modern implementations of languages
such as Lua and JavaScript.
  Why does Python lag so far behind these other languages? As we show, the very
same API and extension libraries that make Python a powerful language also make
it very difficult to efficiently execute. Given that we want to retain access
to the great extension libraries that already exist for Python, how fast can we
make it?
  To evaluate this, we designed and implemented Falcon, a high-performance
bytecode interpreter fully compatible with the standard CPython interpreter.
Falcon applies a number of well known optimizations and introduces several new
techniques to speed up execution of Python bytecode. In our evaluation, we
found Falcon an average of 25% faster than the standard Python interpreter on
most benchmarks and in some cases about 2.5X faster.",http://arxiv.org/abs/1306.6047v2,
Image Processing in Python With Montage,John Good,"[{'@href': 'http://arxiv.org/abs/1908.09753v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1908.09753v1', '@rel': 'related', '@type': 'application/pdf'}]",2019-08-26T15:50:25Z,"The Montage image mosaic engine has found wide applicability in astronomy
research, integration into processing environments, and is an examplar
application for the development of advanced cyber-infrastructure. It is written
in C to provide performance and portability. Linking C/C++ libraries to the
Python kernel at run time as binary extensions allows them to run under Python
at compiled speeds and enables users to take advantage of all the functionality
in Python. We have built Python binary extensions of the 59 ANSI-C modules that
make up version 5 of the Montage toolkit. This has involved a turning the code
into a C library, with driver code fully separated to reproduce the calling
sequence of the command-line tools; and then adding Python and C linkage code
with the Cython library, which acts as a bridge between general C libraries and
the Python interface. We will demonstrate how to use these Python binary
extensions to perform image processing, including reprojecting and resampling
images, rectifying background emission to a common level, creation of image
mosaics that preserve the calibration and astrometric fidelity of the input
images, creating visualizations with an adaptive stretch algorithm, processing
HEALPix images, and analyzing and managing image metadata.",http://arxiv.org/abs/1908.09753v1,
"An Analysis of Python's Topics, Trends, and Technologies Through Mining
  Stack Overflow Discussions",Hamed Tahmooresi,"[{'@href': 'http://arxiv.org/abs/2004.06280v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2004.06280v1', '@rel': 'related', '@type': 'application/pdf'}]",2020-04-14T02:59:16Z,"Python is a popular, widely used, and general-purpose programming language.
In spite of its ever-growing community, researchers have not performed much
analysis on Python's topics, trends, and technologies which provides insights
for developers about Python community trends and main issues. In this article,
we examine the main topics related to this language being discussed by
developers on one of the most popular Q\&A websites, Stack Overflow, as well as
temporal trends through mining 2461876 posts. To be more useful for the
software engineers, we study what Python provides as the alternative to popular
technologies offered by common programming languages like Java. Our results
indicate that discussions about Python standard features, web programming, and
scientific programming. Programming in areas such as mathematics, data science,
statistics, machine learning, natural language processing (NLP), and so forth.
are the most popular areas in the Python community. At the same time, areas
related to scientific programming are steadily receiving more attention from
the Python developers.",http://arxiv.org/abs/2004.06280v1,
Python Workflows on HPC Systems,Dominik Strassel,"[{'@href': 'http://arxiv.org/abs/2012.00365v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2012.00365v1', '@rel': 'related', '@type': 'application/pdf'}]",2020-12-01T09:51:12Z,"The recent successes and wide spread application of compute intensive machine
learning and data analytics methods have been boosting the usage of the Python
programming language on HPC systems. While Python provides many advantages for
the users, it has not been designed with a focus on multi-user environments or
parallel programming - making it quite challenging to maintain stable and
secure Python workflows on a HPC system. In this paper, we analyze the key
problems induced by the usage of Python on HPC clusters and sketch appropriate
workarounds for efficiently maintaining multi-user Python software
environments, securing and restricting resources of Python jobs and containing
Python processes, while focusing on Deep Learning applications running on GPU
clusters.",http://arxiv.org/abs/2012.00365v1,
"Conflict-aware Inference of Python Compatible Runtime Environments with
  Domain Knowledge Graph",Wei Cheng,"[{'@href': 'http://arxiv.org/abs/2201.07029v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2201.07029v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-01-18T14:55:00Z,"Code sharing and reuse is a widespread use practice in software engineering.
Although a vast amount of open-source Python code is accessible on many online
platforms, programmers often find it difficult to restore a successful runtime
environment. Previous studies validated automatic inference of Python
dependencies using pre-built knowledge bases. However, these studies do not
cover sufficient knowledge to accurately match the Python code and also ignore
the potential conflicts between their inferred dependencies, thus resulting in
a low success rate of inference. In this paper, we propose PyCRE, a new
approach to automatically inferring Python compatible runtime environments with
domain knowledge graph (KG). Specifically, we design a domain-specific ontology
for Python third-party packages and construct KGs for over 10,000 popular
packages in Python 2 and Python 3. PyCRE discovers candidate libraries by
measuring the matching degree between the known libraries and the third-party
resources used in target code. For the NP-complete problem of dependency
solving, we propose a heuristic graph traversal algorithm to efficiently
guarantee the compatibility between packages. PyCRE achieves superior
performance on a real-world dataset and efficiently resolves nearly half more
import errors than previous methods.",http://arxiv.org/abs/2201.07029v1,
Triangulating Python Performance Issues with Scalene,Emery D. Berger,"[{'@href': 'http://arxiv.org/abs/2212.07597v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2212.07597v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-12-15T02:56:25Z,"This paper proposes Scalene, a profiler specialized for Python. Scalene
combines a suite of innovations to precisely and simultaneously profile CPU,
memory, and GPU usage, all with low overhead. Scalene's CPU and memory
profilers help Python programmers direct their optimization efforts by
distinguishing between inefficient Python and efficient native execution time
and memory usage. Scalene's memory profiler employs a novel sampling algorithm
that lets it operate with low overhead yet high precision. It also incorporates
a novel algorithm that automatically pinpoints memory leaks, whether within
Python or across the Python-native boundary. Scalene tracks a new metric called
copy volume, which highlights costly copying operations that can occur when
Python silently converts between C and Python data representations, or between
CPU and GPU. Since its introduction, Scalene has been widely adopted, with over
500,000 downloads to date. We present experience reports from developers who
used Scalene to achieve significant performance improvements and memory
savings.",http://arxiv.org/abs/2212.07597v1,
Python bindings for libcloudph++,Dorota Jarecka,"[{'@href': 'http://arxiv.org/abs/1504.01161v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1504.01161v1', '@rel': 'related', '@type': 'application/pdf'}]",2015-04-05T20:58:18Z,"This technical note introduces the Python bindings for libcloudph++. The
libcloudph++ is a C++ library of algorithms for representing atmospheric cloud
microphysics in numerical models. The bindings expose the complete
functionality of the library to the Python users. The bindings are implemented
using the Boost.Python C++ library and use NumPy arrays. This note includes
listings with Python scripts exemplifying the use of selected library
components. An example solution for using the Python bindings to access
libcloudph++ from Fortran is presented.",http://arxiv.org/abs/1504.01161v1,
Yaps: Python Frontend to Stan,Guillaume Baudart,"[{'@href': 'http://arxiv.org/abs/1812.04125v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1812.04125v1', '@rel': 'related', '@type': 'application/pdf'}]",2018-12-06T01:24:29Z,"Stan is a popular probabilistic programming language with a self-contained
syntax and semantics that is close to graphical models. Unfortunately, existing
embeddings of Stan in Python use multi-line strings. That approach forces users
to switch between two different language styles, with no support for syntax
highlighting or simple error reporting within the Stan code. This paper tackles
the question of whether Stan could use Python syntax while retaining its
self-contained semantics. The answer is yes, that can be accomplished by
reinterpreting the Python syntax. This paper introduces Yaps, a new frontend to
Stan based on reinterpreted Python. We tested Yaps on over a thousand Stan
models and made it available open-source.",http://arxiv.org/abs/1812.04125v1,
Python for education: permutations,Andrzej Kapanowski,"[{'@href': 'http://arxiv.org/abs/1307.7042v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1307.7042v1', '@rel': 'related', '@type': 'application/pdf'}]",2013-07-26T14:18:21Z,"Python implementation of permutations is presented. Three classes are
introduced: Perm for permutations, Group for permutation groups, and PermError
to report any errors for both classes. The class Perm is based on Python
dictionaries and utilize cycle notation. The methods of calculation for the
perm order, parity, ranking and unranking are given. A random permutation
generation is also shown. The class Group is very simple and it is also based
on dictionaries. It is mainly the presentation of the permutation groups
interface with methods for the group order, subgroups (normalizer, centralizer,
center, stabilizer), orbits, and several tests. The corresponding Python code
is contained in the modules perms and groups.",http://arxiv.org/abs/1307.7042v1,
Pytrec_eval: An Extremely Fast Python Interface to trec_eval,Christophe Van Gysel,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1145/3209978.3210065', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/1805.01597v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1805.01597v2', '@rel': 'related', '@type': 'application/pdf'}]",2018-05-04T03:37:03Z,"We introduce pytrec_eval, a Python interface to the tree_eval information
retrieval evaluation toolkit. pytrec_eval exposes the reference implementations
of trec_eval within Python as a native extension. We show that pytrec_eval is
around one order of magnitude faster than invoking trec_eval as a sub process
from within Python. Compared to a native Python implementation of NDCG,
pytrec_eval is twice as fast for practically-sized rankings. Finally, we
demonstrate its effectiveness in an application where pytrec_eval is combined
with Pyndri and the OpenAI Gym where query expansion is learned using
Q-learning.",http://arxiv.org/abs/1805.01597v2,
Nonparametric Estimation of the Random Coefficients Model in Python,Emil Mendoza,"[{'@href': 'http://arxiv.org/abs/2108.03582v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2108.03582v2', '@rel': 'related', '@type': 'application/pdf'}]",2021-08-08T07:27:49Z,"We present $\textbf{PyRMLE}$, a Python module that implements Regularized
Maximum Likelihood Estimation for the analysis of Random Coefficient models.
$\textbf{PyRMLE}$ is simple to use and readily works with data formats that are
typical to Random Coefficient problems. The module makes use of Python's
scientific libraries $\textbf{NumPy}$ and $\textbf{SciPy}$ for computational
efficiency. The main implementation of the algorithm is executed purely in
Python code which takes advantage of Python's high-level features.",http://arxiv.org/abs/2108.03582v2,
Running HMC Simulation with Python via QUDA,Shuhei Yamamoto,"[{'@href': 'http://arxiv.org/abs/2212.06657v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2212.06657v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-12-13T15:40:29Z,"Lyncs-API is a Python API for Lattice QCD applications. It is designed as a
Python toolkit that allows the user to use and run various lattice QCD
libraries while programming in Python. The goal is to provide the user an easy
programming experience without scarifying performance across multiple
platforms, by preparing a common framework for various softwares for lattice
QCD calculations. As such, it contains interfaces to, e.g., c-lime, DDalphaAMG,
tmLQCD, and QUDA. In this proceeding, we focus on a Lyncs interface to QUDA,
named Lyncs-QUDA, and present a small tutorial on how to use this Python
interface to perform a HMC simulation using QUDA.",http://arxiv.org/abs/2212.06657v1,
Python client for Isabelle server,Boris Shminke,"[{'@href': 'http://arxiv.org/abs/2212.11173v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2212.11173v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-12-09T12:05:28Z,"We contribute a Python client for the Isabelle server, which gives
researchers and students using Python as their primary programming language an
opportunity to communicate with the Isabelle server through TCP directly from a
Python script. Such an approach helps avoid the complexities of integrating the
existing Python script with languages used for Isabelle development (ML and
Scala). We also describe new features that appeared since the announcement of
the first version of the client a year ago. Finally, we give examples of the
client's applications in research and education and discuss known limitations
and possible directions for future development.",http://arxiv.org/abs/2212.11173v1,
"ePython: An implementation of Python for the many-core Epiphany
  coprocessor",Nick Brown,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1109/PyHPC.2016.012', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2010.14827v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2010.14827v1', '@rel': 'related', '@type': 'application/pdf'}]",2020-10-28T09:01:27Z,"The Epiphany is a many-core, low power, low on-chip memory architecture and
one can very cheaply gain access to a number of parallel cores which is
beneficial for HPC education and prototyping. The very low power nature of
these architectures also means that there is potential for their use in future
HPC machines, however there is a high barrier to entry in programming them due
to the associated complexities and immaturity of supporting tools.
  In this paper we present our work on ePython, a subset of Python for the
Epiphany and similar many-core co-processors. Due to the limited on-chip memory
per core we have developed a new Python interpreter and this, combined with
additional support for parallelism, has meant that novices can take advantage
of Python to very quickly write parallel codes on the Epiphany and explore
concepts of HPC using a smaller scale parallel machine. The high level nature
of Python opens up new possibilities on the Epiphany, we examine a
computationally intensive Gauss-Seidel code from the programmability and
performance perspective, discuss running Python hybrid on both the host CPU and
Epiphany, and interoperability between a full Python interpreter on the CPU and
ePython on the Epiphany. The result of this work is support for developing
Python on the Epiphany, which can be applied to other similar architectures,
that the community have already started to adopt and use to explore concepts of
parallelism and HPC.",http://arxiv.org/abs/2010.14827v1,
Characterizing Bugs in Python and R Data Analytics Programs,Shibbir Ahmed,"[{'@href': 'http://arxiv.org/abs/2306.08632v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2306.08632v1', '@rel': 'related', '@type': 'application/pdf'}]",2023-06-14T16:50:01Z,"R and Python are among the most popular languages used in many critical data
analytics tasks. However, we still do not fully understand the capabilities of
these two languages w.r.t. bugs encountered in data analytics tasks. What type
of bugs are common? What are the main root causes? What is the relation between
bugs and root causes? How to mitigate these bugs? We present a comprehensive
study of 5,068 Stack Overflow posts, 1,800 bug fix commits from GitHub
repositories, and several GitHub issues of the most used libraries to
understand bugs in R and Python. Our key findings include: while both R and
Python have bugs due to inexperience with data analysis, Python see
significantly larger data preprocessing bugs compared to R. Developers
experience significantly more data flow bugs in R because intermediate results
are often implicit. We also found changes and bugs in packages and libraries
cause more bugs in R compared to Python while package or library misselection
and conflicts cause more bugs in Python than R. While R has a slightly higher
readability barrier for data analysts, the statistical power of R leads to a
less number of bad performance bugs. In terms of data visualization, R packages
have significantly more bugs than Python libraries. We also identified a strong
correlation between comparable packages in R and Python despite their
linguistic and methodological differences. Lastly, we contribute a large
dataset of manually verified R and Python bugs.",http://arxiv.org/abs/2306.08632v1,
"Simplifying Parallelization of Scientific Codes by a Function-Centric
  Approach in Python",Jon K. Nilsen,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1088/1749-4699/3/1/015003', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/1002.0705v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1002.0705v1', '@rel': 'related', '@type': 'application/pdf'}]",2010-02-03T12:31:14Z,"The purpose of this paper is to show how existing scientific software can be
parallelized using a separate thin layer of Python code where all parallel
communication is implemented. We provide specific examples on such layers of
code, and these examples may act as templates for parallelizing a wide set of
serial scientific codes. The use of Python for parallelization is motivated by
the fact that the language is well suited for reusing existing serial codes
programmed in other languages. The extreme flexibility of Python with regard to
handling functions makes it very easy to wrap up decomposed computational tasks
of a serial scientific application as Python functions. Many
parallelization-specific components can be implemented as generic Python
functions, which may take as input those functions that perform concrete
computational tasks. The overall programming effort needed by this
parallelization approach is rather limited, and the resulting parallel Python
scripts have a compact and clean structure. The usefulness of the
parallelization approach is exemplified by three different classes of
applications in natural and social sciences.",http://arxiv.org/abs/1002.0705v1,
"DockerizeMe: Automatic Inference of Environment Dependencies for Python
  Code Snippets",Eric Horton,"[{'@href': 'http://arxiv.org/abs/1905.11127v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1905.11127v1', '@rel': 'related', '@type': 'application/pdf'}]",2019-05-27T11:23:29Z,"Platforms like Stack Overflow and GitHub's gist system promote the sharing of
ideas and programming techniques via the distribution of code snippets designed
to illustrate particular tasks. Python, a popular and fast-growing programming
language, sees heavy use on both sites, with nearly one million questions asked
on Stack Overflow and 400 thousand public gists on GitHub. Unfortunately,
around 75% of the Python example code shared through these sites cannot be
directly executed. When run in a clean environment, over 50% of public Python
gists fail due to an import error for a missing library.
  We present DockerizeMe, a technique for inferring the dependencies needed to
execute a Python code snippet without import error. DockerizeMe starts with
offline knowledge acquisition of the resources and dependencies for popular
Python packages from the Python Package Index (PyPI). It then builds Docker
specifications using a graph-based inference procedure. Our inference procedure
resolves import errors in 892 out of nearly 3,000 gists from the Gistable
dataset for which Gistable's baseline approach could not find and install all
dependencies.",http://arxiv.org/abs/1905.11127v1,
OpenML-Python: an extensible Python API for OpenML,Matthias Feurer,"[{'@href': 'http://arxiv.org/abs/1911.02490v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1911.02490v2', '@rel': 'related', '@type': 'application/pdf'}]",2019-11-06T16:59:30Z,"OpenML is an online platform for open science collaboration in machine
learning, used to share datasets and results of machine learning experiments.
In this paper we introduce OpenML-Python, a client API for Python, opening up
the OpenML platform for a wide range of Python-based tools. It provides easy
access to all datasets, tasks and experiments on OpenML from within Python. It
also provides functionality to conduct machine learning experiments, upload the
results to OpenML, and reproduce results which are stored on OpenML.
Furthermore, it comes with a scikit-learn plugin and a plugin mechanism to
easily integrate other machine learning libraries written in Python into the
OpenML ecosystem. Source code and documentation is available at
https://github.com/openml/openml-python/.",http://arxiv.org/abs/1911.02490v2,
"Fast fully-reproducible serial/parallel Monte Carlo and MCMC simulations
  and visualizations via ParaMonte::Python library",Amir Shahmoradi,"[{'@href': 'http://arxiv.org/abs/2010.00724v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2010.00724v1', '@rel': 'related', '@type': 'application/pdf'}]",2020-10-01T23:26:42Z,"ParaMonte::Python (standing for Parallel Monte Carlo in Python) is a serial
and MPI-parallelized library of (Markov Chain) Monte Carlo (MCMC) routines for
sampling mathematical objective functions, in particular, the posterior
distributions of parameters in Bayesian modeling and analysis in data science,
Machine Learning, and scientific inference in general. In addition to providing
access to fast high-performance serial/parallel Monte Carlo and MCMC sampling
routines, the ParaMonte::Python library provides extensive post-processing and
visualization tools that aim to automate and streamline the process of model
calibration and uncertainty quantification in Bayesian data analysis.
Furthermore, the automatically-enabled restart functionality of
ParaMonte::Python samplers ensure seamless fully-deterministic into-the-future
restart of Monte Carlo simulations, should any interruptions happen. The
ParaMonte::Python library is MIT-licensed and is permanently maintained on
GitHub at
https://github.com/cdslaborg/paramonte/tree/master/src/interface/Python.",http://arxiv.org/abs/2010.00724v1,
"Productivity, Portability, Performance: Data-Centric Python",Alexandros Nikolaos Ziogas,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1145/1122445.1122456', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2107.00555v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2107.00555v2', '@rel': 'related', '@type': 'application/pdf'}]",2021-07-01T15:51:18Z,"Python has become the de facto language for scientific computing. Programming
in Python is highly productive, mainly due to its rich science-oriented
software ecosystem built around the NumPy module. As a result, the demand for
Python support in High Performance Computing (HPC) has skyrocketed. However,
the Python language itself does not necessarily offer high performance. In this
work, we present a workflow that retains Python's high productivity while
achieving portable performance across different architectures. The workflow's
key features are HPC-oriented language extensions and a set of automatic
optimizations powered by a data-centric intermediate representation. We show
performance results and scaling across CPU, GPU, FPGA, and the Piz Daint
supercomputer (up to 23,328 cores), with 2.47x and 3.75x speedups over
previous-best solutions, first-ever Xilinx and Intel FPGA results of annotated
Python, and up to 93.16% scaling efficiency on 512 nodes.",http://arxiv.org/abs/2107.00555v2,
PyTracer: Automatically profiling numerical instabilities in Python,Yohan Chatelain,"[{'@href': 'http://arxiv.org/abs/2112.11508v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2112.11508v2', '@rel': 'related', '@type': 'application/pdf'}]",2021-12-21T20:22:34Z,"Numerical stability is a crucial requirement of reliable scientific
computing. However, despite the pervasiveness of Python in data science,
analyzing large Python programs remains challenging due to the lack of scalable
numerical analysis tools available for this language. To fill this gap, we
developed PyTracer, a profiler to quantify numerical instability in Python
applications. PyTracer transparently instruments Python code to produce
numerical traces and visualize them interactively in a Plotly dashboard. We
designed PyTracer to be agnostic to numerical noise model, allowing for tool
evaluation through Monte-Carlo Arithmetic, random rounding, random data
perturbation, or structured noise for a particular application. We illustrate
PyTracer's capabilities by testing the numerical stability of key functions in
both SciPy and Scikit-learn, two dominant Python libraries for mathematical
modeling. Through these evaluations, we demonstrate PyTracer as a scalable,
automatic, and generic framework for numerical profiling in Python.",http://arxiv.org/abs/2112.11508v2,
GAP-Gen: Guided Automatic Python Code Generation,Junchen Zhao,"[{'@href': 'http://arxiv.org/abs/2201.08810v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2201.08810v2', '@rel': 'related', '@type': 'application/pdf'}]",2022-01-19T06:32:47Z,"Automatic code generation from natural language descriptions can be highly
beneficial during the process of software development. In this work, we propose
GAP-Gen, a Guided Automatic Python Code Generation method based on Python
syntactic constraints and semantic constraints. We first introduce Python
syntactic constraints in the form of Syntax-Flow, which is a simplified version
of Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract
Syntax Tree but maintaining crucial syntactic information of Python code. In
addition to Syntax-Flow, we introduce Variable-Flow which abstracts variable
and function names consistently through out the code. In our work, rather than
pretraining, we focus on modifying the finetuning process which reduces
computational requirements but retains high generation performance on automatic
Python code generation task. GAP-Gen fine-tunes the transformer based language
models T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet,
CodeSearchNet AdvTest and Code-Docstring Corpus from EdinburghNLP. Our
experiments show that GAP-Gen achieves better results on automatic Python code
generation task than previous works.",http://arxiv.org/abs/2201.08810v2,
Approaches to the Parallelization of Merge Sort in Python,Alexandra Yang,"[{'@href': 'http://arxiv.org/abs/2211.16479v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2211.16479v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-11-26T18:26:30Z,"The theory of divide-and-conquer parallelization has been well-studied in the
past, providing a solid basis upon which to explore different approaches to the
parallelization of merge sort in Python. Python's simplicity and extensive
selection of libraries make it the most popular scientific programming
language, so it is a fitting language in which to implement and analyze these
algorithms.
  In this paper, we use Python packages multiprocessing and mpi4py to implement
several different parallel merge sort algorithms. Experiments are conducted on
an academic supercomputer, upon which benchmarks are performed using Cloudmesh.
We find that hybrid multiprocessing merge sort outperforms several other
algorithms, achieving a 1.5x speedup compared to the built-in Python sorted()
and a 34x speedup compared to sequential merge sort. Our results provide
insight into different approaches to implementing parallel merge sort in Python
and contribute to the understanding of general divide-and-conquer
parallelization in Python on both shared and distributed memory systems.",http://arxiv.org/abs/2211.16479v1,
"Faster or Slower? Performance Mystery of Python Idioms Unveiled with
  Empirical Evidence",Zejun Zhang,"[{'@href': 'http://arxiv.org/abs/2301.12633v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2301.12633v1', '@rel': 'related', '@type': 'application/pdf'}]",2023-01-30T03:28:24Z,"The usage of Python idioms is popular among Python developers in a formative
study of 101 performance-related questions of Python idioms on Stack Overflow,
we find that developers often get confused about the performance impact of
Python idioms and use anecdotal toy code or rely on personal project experience
which is often contradictory in performance outcomes. There has been no
large-scale, systematic empirical evidence to reconcile these performance
debates. In the paper, we create a large synthetic dataset with 24,126 pairs of
non-idiomatic and functionally-equivalent idiomatic code for the nine unique
Python idioms identified in Zhang et al., and reuse a large real-project
dataset of 54,879 such code pairs provided by Zhang et al. We develop a
reliable performance measurement method to compare the speedup or slowdown by
idiomatic code against non-idiomatic counterpart, and analyze the performance
discrepancies between the synthetic and real-project code, the relationships
between code features and performance changes, and the root causes of
performance changes at the bytecode level. We summarize our findings as some
actionable suggestions for using Python idioms.",http://arxiv.org/abs/2301.12633v1,
Rapid Development of Interferometric Software Using MIRIAD and Python,Peter K. G. Williams,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1086/666604', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/1203.0330v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1203.0330v1', '@rel': 'related', '@type': 'application/pdf'}]",2012-03-01T22:07:31Z,"New and upgraded radio interferometers produce data at massive rates and will
require significant improvements in analysis techniques to reach their promised
levels of performance in a routine manner. Until these techniques are fully
developed, productivity and accessibility in scientific programming
environments will be key bottlenecks in the pipeline leading from data-taking
to research results. We present an open-source software package, miriad-python,
that allows access to the MIRIAD interferometric reduction system in the Python
programming language. The modular design of MIRIAD and the high productivity
and accessibility of Python provide an excellent foundation for rapid
development of interferometric software. Several other projects with similar
goals exist and we describe them and compare miriad-python to them in detail.
Along with an overview of the package design, we present sample code and
applications, including the detection of millisecond astrophysical transients,
determination and application of nonstandard calibration parameters,
interactive data visualization, and a reduction pipeline using a directed
acyclic graph dependency model analogous to that of the traditional Unix tool
""make"". The key aspects of the miriad-python software project are documented.
We find that miriad-python provides an extremely effective environment for
prototyping new interferometric software, though certain existing packages
provide far more infrastructure for some applications. While equivalent
software written in compiled languages can be much faster than Python, there
are many situations in which execution time is profitably exchanged for speed
of development, code readability, accessibility to nonexpert programmers, quick
interlinking with foreign software packages, and other virtues of the Python
language.",http://arxiv.org/abs/1203.0330v1,
Toward Efficient Interactions between Python and Native Libraries,Jialiang Tan,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1145/3468264.3468541', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2107.00064v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2107.00064v1', '@rel': 'related', '@type': 'application/pdf'}]",2021-06-11T00:48:02Z,"Python has become a popular programming language because of its excellent
programmability. Many modern software packages utilize Python for high-level
algorithm design and depend on native libraries written in C/C++/Fortran for
efficient computation kernels. Interaction between Python code and native
libraries introduces performance losses because of the abstraction lying on the
boundary of Python and native libraries. On the one side, Python code,
typically run with interpretation, is disjoint from its execution behavior. On
the other side, native libraries do not include program semantics to understand
algorithm defects.
  To understand the interaction inefficiencies, we extensively study a large
collection of Python software packages and categorize them according to the
root causes of inefficiencies. We extract two inefficiency patterns that are
common in interaction inefficiencies. Based on these patterns, we develop
PieProf, a lightweight profiler, to pinpoint interaction inefficiencies in
Python applications. The principle of PieProf is to measure the inefficiencies
in the native execution and associate inefficiencies with high-level Python
code to provide a holistic view. Guided by PieProf, we optimize 17 real-world
applications, yielding speedups up to 6.3$\times$ on application level.",http://arxiv.org/abs/2107.00064v1,
"Improving Tese Case Generation for Python Native Libraries Through
  Constraints on Input Data Structures",Xin Zhang,"[{'@href': 'http://arxiv.org/abs/2206.13828v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2206.13828v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-06-28T08:47:33Z,"Modern Python projects execute computational functions using native libraries
and give Python interfaces to boost execution speed; hence, testing these
libraries becomes critical to the project's robustness. One challenge is that
existing approaches use coverage to guide generation, but native libraries run
as black boxes to Python code with no execution information. Another is that
dynamic binary instrumentation reduces testing performance as it needs to
monitor both native libraries and the Python virtual machine.
  To address these challenges, in this paper, we propose an automated test case
generation approach that works at the Python code layer. Our insight is that
many path conditions in native libraries are for processing input data
structures through interacting with the VM. In our approach, we instrument the
Python Interpreter to monitor the interactions between native libraries and VM,
derive constraints on the structures, and then use the constraints to guide
test case generation. We implement our approach in a tool named PyCing and
apply it to six widely-used Python projects. The experimental results reveal
that with the structure constraint guidance, PyCing can cover more execution
paths than existing test cases and state-of-the-art tools. Also, with the
checkers in the testing framework Pytest, PyCing can identify segmentation
faults in 10 Python interfaces and memory leaks in 9. Our instrumentation
strategy also has an acceptable influence on testing efficiency.",http://arxiv.org/abs/2206.13828v1,
"binary_c-python: A Python-based stellar population synthesis tool and
  interface to binary_c",D. D. Hendriks,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.21105/joss.04642', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2306.02779v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2306.02779v1', '@rel': 'related', '@type': 'application/pdf'}]",2023-06-05T11:04:17Z,"We present the software package binary_c-python which provides a convenient
and easy-to-use interface to the binary_c framework, allowing the user to
rapidly evolve individual systems and populations of stars. binary_c-python is
available on Pip and on GitLab. binary_c-python contains many useful features
to control and process the output of binary_c, like by providing
binary_c-python with logging statements that are dynamically compiled and
loaded into binary_c. Moreover, we have recently added standardised output of
events like Roche-lobe overflow or double compact-object formation to binary_c,
and automatic parsing and managing of that output in binary_c-python.
binary_c-python uses multiprocessing to utilise all the cores on a particular
machine, and can run populations with HPC cluster workload managers like
HTCondor and Slurm, allowing the user to run simulations on large computing
clusters. We provide documentation that is automatically generated based on
docstrings and a suite of Jupyter notebooks. These notebooks consist of
technical tutorials on how to use binary_c-python and use-case scenarios aimed
at doing science. Much of binary_c-python is covered by unit tests to ensure
reliability and correctness, and the test coverage is continually increased as
the package is improved.",http://arxiv.org/abs/2306.02779v1,
"PyMsOfa: A Python Package for the Standards of Fundamental Astronomy
  (SOFA) Service",Jianghui Ji,"[{'@href': 'http://arxiv.org/abs/2310.08673v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2310.08673v2', '@rel': 'related', '@type': 'application/pdf'}]",2023-10-12T19:11:41Z,"The Standards of Fundamental Astronomy (SOFA) is a service provided by the
International Astronomical Union (IAU) that offers algorithms and software for
astronomical calculations, which was released in two versions by FORTRAN 77 and
ANSI C, respectively. In this work, we implement the python package PyMsOfa for
SOFA service by three ways: (1) a python wrapper package based on a foreign
function library for Python (ctypes), (2) a python wrapper package with the
foreign function interface for Python calling C code (cffi), and (3) a python
package directly written in pure python codes from SOFA subroutines. The
package PyMsOfa has fully implemented 247 functions of the original SOFA
routines. In addition, PyMsOfa is also extensively examined, which is exactly
consistent with those test examples given by the original SOFA. This python
package can be suitable to not only the astrometric detection of habitable
planets of the Closeby Habitable Exoplanet Survey (CHES) mission (Ji et al.
2022), but also for the frontiers themes of black holes and dark matter related
to astrometric calculations and other fields. The source codes are available
via https://github.com/CHES2023/PyMsOfa.",http://arxiv.org/abs/2310.08673v2,
Python for Education: Computational Methods for Nonlinear Systems,Christopher R. Myers,"[{'@href': 'http://arxiv.org/abs/0704.3182v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/0704.3182v1', '@rel': 'related', '@type': 'application/pdf'}]",2007-04-24T18:55:17Z,"We describe a novel, interdisciplinary, computational methods course that
uses Python and associated numerical and visualization libraries to enable
students to implement simulations for a number of different course modules.
Problems in complex networks, biomechanics, pattern formation, and gene
regulation are highlighted to illustrate the breadth and flexibility of
Python-powered computational environments.",http://arxiv.org/abs/0704.3182v1,
Implementation of Kalman Filter with Python Language,Mohamed Laaraiedh,"[{'@href': 'http://arxiv.org/abs/1204.0375v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1204.0375v1', '@rel': 'related', '@type': 'application/pdf'}]",2012-04-02T11:40:41Z,"In this paper, we investigate the implementation of a Python code for a
Kalman Filter using the Numpy package. A Kalman Filtering is carried out in two
steps: Prediction and Update. Each step is investigated and coded as a function
with matrix input and output. These different functions are explained and an
example of a Kalman Filter application for the localization of mobile in
wireless networks is given.",http://arxiv.org/abs/1204.0375v1,
A Framework for Distributed Deep Learning Layer Design in Python,Clay McLeod,"[{'@href': 'http://arxiv.org/abs/1510.07303v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1510.07303v1', '@rel': 'related', '@type': 'application/pdf'}]",2015-10-25T21:04:12Z,"In this paper, a framework for testing Deep Neural Network (DNN) design in
Python is presented. First, big data, machine learning (ML), and Artificial
Neural Networks (ANNs) are discussed to familiarize the reader with the
importance of such a system. Next, the benefits and detriments of implementing
such a system in Python are presented. Lastly, the specifics of the system are
explained, and some experimental results are presented to prove the
effectiveness of the system.",http://arxiv.org/abs/1510.07303v1,
Want Drugs? Use Python,Michał Nowotka,"[{'@href': 'http://arxiv.org/abs/1607.00378v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1607.00378v1', '@rel': 'related', '@type': 'application/pdf'}]",2016-07-01T19:02:36Z,"We describe how Python can be leveraged to streamline the curation, modelling
and dissemination of drug discovery data as well as the development of
innovative, freely available tools for the related scientific community. We
look at various examples, such as chemistry toolkits, machine-learning
applications and web frameworks and show how Python can glue it all together to
create efficient data science pipelines.",http://arxiv.org/abs/1607.00378v1,
Geoplotlib: a Python Toolbox for Visualizing Geographical Data,Andrea Cuttone,"[{'@href': 'http://arxiv.org/abs/1608.01933v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1608.01933v1', '@rel': 'related', '@type': 'application/pdf'}]",2016-08-05T16:39:27Z,"We introduce geoplotlib, an open-source python toolbox for visualizing
geographical data. geoplotlib supports the development of hardware-accelerated
interactive visualizations in pure python, and provides implementations of dot
maps, kernel density estimation, spatial graphs, Voronoi tesselation,
shapefiles and many more common spatial visualizations. We describe geoplotlib
design, functionalities and use cases.",http://arxiv.org/abs/1608.01933v1,
"Powerbox: A Python package for creating structured fields with isotropic
  power spectra",Steven G. Murray,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.21105/joss.00850', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/1809.05030v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1809.05030v1', '@rel': 'related', '@type': 'application/pdf'}]",2018-08-27T00:21:57Z,"Powerbox is a pure-Python package for creating and measuring structured
fields with homogeneous and isotropic power spectra.",http://arxiv.org/abs/1809.05030v1,
TimeGym: Debugging for Time Series Modeling in Python,Diogo Seca,"[{'@href': 'http://arxiv.org/abs/2105.01404v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2105.01404v1', '@rel': 'related', '@type': 'application/pdf'}]",2021-05-04T10:36:29Z,"We introduce the TimeGym Forecasting Debugging Toolkit, a Python library for
testing and debugging time series forecasting pipelines. TimeGym simplifies the
testing forecasting pipeline by providing generic tests for forecasting
pipelines fresh out of the box. These tests are based on common modeling
challenges of time series. Our library enables forecasters to apply a
Test-Driven Development approach to forecast modeling, using specified oracles
to generate artificial data with noise.",http://arxiv.org/abs/2105.01404v1,
MontePython: Implementing Quantum Monte Carlo using Python,J. K. Nilsen,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1016/j.cpc.2007.06.013', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/physics/0609191v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/physics/0609191v1', '@rel': 'related', '@type': 'application/pdf'}]",2006-09-22T12:42:42Z,"We present a cross-language C++/Python program for simulations of quantum
mechanical systems with the use of Quantum Monte Carlo (QMC) methods. We
describe a system for which to apply QMC, the algorithms of variational Monte
Carlo and diffusion Monte Carlo and we describe how to implement theses methods
in pure C++ and C++/Python. Furthermore we check the efficiency of the
implementations in serial and parallel cases to show that the overhead using
Python can be negligible.",http://arxiv.org/abs/physics/0609191v1,
"HOOMD-blue: A Python package for high-performance molecular dynamics and
  hard particle Monte Carlo simulations",Joshua A. Anderson,"[{'@href': 'http://arxiv.org/abs/1308.5587v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1308.5587v2', '@rel': 'related', '@type': 'application/pdf'}]",2013-08-26T13:56:04Z,"HOOMD-blue is a particle simulation engine designed for nano- and
colloidal-scale molecular dynamics and hard particle Monte Carlo simulations.
It has been actively developed since March 2007 and available open source since
August 2008. HOOMD-blue is a Python package with a high performance C++/CUDA
backend that we built from the ground up for GPU acceleration. The Python
interface allows users to combine HOOMD-blue with with other packages in the
Python ecosystem to create simulation and analysis workflows. We employ
software engineering practices to develop, test, maintain, and expand the code.",http://arxiv.org/abs/1308.5587v2,
Plyades: A Python Library for Space Mission Design,Helge Eichhorn,"[{'@href': 'http://arxiv.org/abs/1607.00849v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1607.00849v1', '@rel': 'related', '@type': 'application/pdf'}]",2016-07-01T18:53:15Z,"Plyades: A Python Library for Space Mission Design Designing a space mission
is a computation-heavy task. Software tools that conduct the necessary
numerical simulations and optimizations are therefore indispensable. The
usability of existing software, written in Fortran and MATLAB, suffers because
of high complexity, low levels of abstraction and out-dated programming
practices. We propose Python as a viable alternative for astrodynamics tools
and demonstrate the proof-of-concept library Plyades which combines powerful
features with Pythonic ease of use.",http://arxiv.org/abs/1607.00849v1,
A Practical Python API for Querying AFLOWLIB,Conred W. Rosenbrock,"[{'@href': 'http://arxiv.org/abs/1710.00813v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1710.00813v1', '@rel': 'related', '@type': 'application/pdf'}]",2017-09-28T20:38:47Z,"Large databases such as aflowlib.org provide valuable data sources for
discovering material trends through machine learning. Although a REST API and
query language are available, there is a learning curve associated with the
AFLUX language that acts as a barrier for new users. Additionally, the data is
stored using non-standard serialization formats. Here we present a high-level
API that allows immediate access to the aflowlib data using standard python
operators and language features. It provides an easy way to integrate aflowlib
data with other python materials packages such as ase and quippy, and provides
automatic deserialization into numpy arrays and python objects. This package is
available via ""pip install aflow"".",http://arxiv.org/abs/1710.00813v1,
salmon: A Symbolic Linear Regression Package for Python,Alex Boyd,"[{'@href': 'http://arxiv.org/abs/1911.00648v3', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1911.00648v3', '@rel': 'related', '@type': 'application/pdf'}]",2019-11-02T04:42:28Z,"One of the most attractive features of R is its linear modeling capabilities.
We describe a Python package, salmon, that brings the best of R's linear
modeling functionality to Python in a Pythonic way -- by providing composable
objects for specifying and fitting linear models. This object-oriented design
also enables other features that enhance ease-of-use, such as automatic
visualizations and intelligent model building.",http://arxiv.org/abs/1911.00648v3,
pySiDR: Python Event Reconstruction for SiD,C. T. Potter,"[{'@href': 'http://arxiv.org/abs/2002.05804v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2002.05804v1', '@rel': 'related', '@type': 'application/pdf'}]",2020-02-13T22:43:25Z,"Event reconstruction in the ILC community has typically relied on algorithms
implemented in C++, a fast compiled language. However, the Python package
pyLCIO provides a full interface to tracker and calorimeter hits stored in LCIO
files, opening up the possibility to implement reconstruction algorithms in a
language uniquely well suited to working with large lists of hits built with
list comprehensions. Python, an interpreted language which can perform complex
tasks with minimal code, also allows seamless integration with powerful machine
learning tools developed recently. We discuss pySiDR, a Python package for SiD
event reconstruction.",http://arxiv.org/abs/2002.05804v1,
"FitsGeo -- Python package for PHITS geometry development and
  visualization",Ivan Gordeev,"[{'@href': 'http://arxiv.org/abs/2008.03298v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2008.03298v1', '@rel': 'related', '@type': 'application/pdf'}]",2020-08-08T09:54:21Z,"An easy way to define and visualize geometry for PHITS input files
introduced. Suggested FitsGeo Python package helps to define surfaces as Python
objects and manipulate them conveniently. VPython assists to view defined
geometry interactively which boosts geometry development and helps with
complicated cases. Every class that sets the surface object has methods with
some extra properties. As well as geometry generation for PHITS input,
additional modules developed for material and cell definition. Any user with a
very basic knowledge of Python can define the geometry in a convenient way and
use it in further research related to particle transport.",http://arxiv.org/abs/2008.03298v1,
HDPython: A High Level Python Based Object-Oriented HDL Framework,R. Peschke,"[{'@href': 'http://arxiv.org/abs/2011.02626v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2011.02626v2', '@rel': 'related', '@type': 'application/pdf'}]",2020-11-05T02:43:50Z,"We present a High-Level Python-based Hardware Description Language
(HDPython), It uses Python as its source language and converts it to standard
VHDL. Compared to other approaches of building converters from a high-level
programming language into a hardware description language, this new approach
aims to maintain an object-oriented paradigm throughout the entire process.
Instead of removing all the high-level features from Python to make it into an
HDL, this approach goes the opposite way. It tries to show how certain features
from a high-level language can be implemented in an HDL, providing the
corresponding benefits of high-level programming for the user.",http://arxiv.org/abs/2011.02626v2,
cellanneal: A User-Friendly Deconvolution Software for Omics Data,Lisa Buchauer,"[{'@href': 'http://arxiv.org/abs/2110.08209v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2110.08209v1', '@rel': 'related', '@type': 'application/pdf'}]",2021-10-15T17:14:58Z,"We introduce cellanneal, a python-based software for deconvolving bulk RNA
sequencing data. cellanneal relies on the optimization of Spearman's rank
correlation coefficient between experimental and computational mixture gene
expression vectors using simulated annealing. cellanneal can be used as a
python package or via a command line interface, but importantly also provides a
simple graphical user interface which is distributed as a single executable
file for user convenience. The python package is available at
https://github.com/LiBuchauer/cellanneal , the graphical software can be
downloaded at http://shalevlab.weizmann.ac.il/resources .",http://arxiv.org/abs/2110.08209v1,
Asgl: A Python Package for Penalized Linear and Quantile Regression,Álvaro Méndez Civieta,"[{'@href': 'http://arxiv.org/abs/2111.00472v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2111.00472v1', '@rel': 'related', '@type': 'application/pdf'}]",2021-10-31T11:43:10Z,"Asg is a Python package that solves penalized linear regression and quantile
regression models for simultaneous variable selection and prediction, for both
high and low dimensional frameworks. It makes very easy to set up and solve
different types of lasso-based penalizations among which the asgl (adaptive
sparse group lasso, that gives name to the package) is remarked. This package
is built on top of cvxpy, a Python-embedded modeling language for convex
optimization problems and makes extensive use of multiprocessing, a Python
module for parallel computing that significantly reduces computation times of
asgl.",http://arxiv.org/abs/2111.00472v1,
Scalpel: The Python Static Analysis Framework,Li Li,"[{'@href': 'http://arxiv.org/abs/2202.11840v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2202.11840v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-02-24T00:27:56Z,"Despite being the most popular programming language, Python has not yet
received enough attention from the community. To the best of our knowledge,
there is no general static analysis framework proposed to facilitate the
implementation of dedicated Python static analyzers. To fill this gap, we
design and implement such a framework (named Scalpel) and make it publicly
available as an open-source project. The Scalpel framework has already
integrated a number of fundamental static analysis functions (e.g., call graph
constructions, control-flow graph constructions, alias analysis, etc.) that are
ready to be reused by developers to implement client applications focusing on
statically resolving dedicated Python problems such as detecting bugs or fixing
vulnerabilities.",http://arxiv.org/abs/2202.11840v1,
"Modernizing the ESRF beamline application software architecture with
  generic Python modules",Jorg Klora,"[{'@href': 'http://arxiv.org/abs/cond-mat/0210344v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/cond-mat/0210344v1', '@rel': 'related', '@type': 'application/pdf'}]",2002-10-16T13:27:22Z,"We report on the modernization of the ESRF beamline application software with
Python modules. The current building blocks used around the SPEC data
acquisition software together with the new elements are presented.",http://arxiv.org/abs/cond-mat/0210344v1,
Multi-Agent Programming Contest 2011 - The Python-DTU Team,Jørgen Villadsen,"[{'@href': 'http://arxiv.org/abs/1110.0105v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1110.0105v1', '@rel': 'related', '@type': 'application/pdf'}]",2011-10-01T15:03:52Z,"We provide a brief description of the Python-DTU system, including the
overall design, the tools and the algorithms that we plan to use in the agent
contest.",http://arxiv.org/abs/1110.0105v1,
"Proceedings of the 6th European Conference on Python in Science
  (EuroSciPy 2013)",Pierre de Buyl,"[{'@href': 'http://arxiv.org/abs/1405.0166v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1405.0166v1', '@rel': 'related', '@type': 'application/pdf'}]",2014-05-01T14:22:09Z,"These are the proceedings of the 6th European Conference on Python in
Science, EuroSciPy 2013, that was held in Brussels (21-25 August 2013).",http://arxiv.org/abs/1405.0166v1,
Py-oopsi: the python implementation of the fast-oopsi algorithm,Benyuan Liu,"[{'@href': 'http://arxiv.org/abs/1405.6181v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1405.6181v1', '@rel': 'related', '@type': 'application/pdf'}]",2014-05-06T06:05:04Z,"Fast-oopsi was developed by Joshua Vogelstein in 2009, which is now widely
used to extract neuron spike activities from calcium fluorescence signals.
Here, we propose detailed implementation of the fast-oopsi algorithm in python
programming language. Some corrections are also made to the original fast-oopsi
paper.",http://arxiv.org/abs/1405.6181v1,
"Proceedings of the 7th European Conference on Python in Science
  (EuroSciPy 2014)",Pierre de Buyl,"[{'@href': 'http://arxiv.org/abs/1412.7030v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1412.7030v1', '@rel': 'related', '@type': 'application/pdf'}]",2014-12-22T15:47:51Z,"These are the proceedings of the 7th European Conference on Python in
Science, EuroSciPy 2014, that was held in Cambridge, UK (27-30 August 2014).",http://arxiv.org/abs/1412.7030v1,
PythonFOAM: In-situ data analyses with OpenFOAM and Python,Romit Maulik,"[{'@href': 'http://arxiv.org/abs/2103.09389v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2103.09389v2', '@rel': 'related', '@type': 'application/pdf'}]",2021-03-17T01:34:41Z,"We outline the development of a general-purpose Python-based data analysis
tool for OpenFOAM. Our implementation relies on the construction of OpenFOAM
applications that have bindings to data analysis libraries in Python. Double
precision data in OpenFOAM is cast to a NumPy array using the NumPy C-API and
Python modules may then be used for arbitrary data analysis and manipulation on
flow-field information. We highlight how the proposed wrapper may be used for
an in-situ online singular value decomposition (SVD) implemented in Python and
accessed from the OpenFOAM solver PimpleFOAM. Here, `in-situ' refers to a
programming paradigm that allows for a concurrent computation of the data
analysis on the same computational resources utilized for the partial
differential equation solver. In addition, to demonstrate data-parallel
analyses, we deploy a distributed SVD, which collects snapshot data across the
ranks of a distributed simulation to compute the global left singular vectors.
Crucially, both OpenFOAM and Python share the same message passing interface
(MPI) communicator for this deployment which allows Python objects and
functions to exchange NumPy arrays across ranks. Subsequently, we provide
scaling assessments of this distributed SVD on multiple nodes of Intel
Broadwell and KNL architectures for canonical test cases such as the large eddy
simulations of a backward facing step and a channel flow at friction Reynolds
number of 395. Finally, we demonstrate the deployment of a deep neural network
for compressing the flow-field information using an autoencoder to demonstrate
an ability to use state-of-the-art machine learning tools in the Python
ecosystem.",http://arxiv.org/abs/2103.09389v2,
Python Crypto Misuses in the Wild,Anna-Katharina Wickert,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1145/3475716.3484195', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2109.01109v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2109.01109v1', '@rel': 'related', '@type': 'application/pdf'}]",2021-09-02T17:32:41Z,"Background: Previous studies have shown that up to 99.59 % of the Java apps
using crypto APIs misuse the API at least once. However, these studies have
been conducted on Java and C, while empirical studies for other languages are
missing. For example, a controlled user study with crypto tasks in Python has
shown that 68.5 % of the professional developers write a secure solution for a
crypto task. Aims: To understand if this observation holds for real-world code,
we conducted a study of crypto misuses in Python. Method: We developed a static
analysis tool that covers common misuses of 5 different Python crypto APIs.
With this analysis, we analyzed 895 popular Python projects from GitHub and 51
MicroPython projects for embedded devices. Further, we compared our results
with the findings of previous studies. Results: Our analysis reveals that 52.26
% of the Python projects have at least one misuse. Further, some Python crypto
libraries API design helps developers from misusing crypto functions, which
were much more common in studies conducted with Java and C code. Conclusion: We
conclude that we can see a positive impact of the good API design on crypto
misuses for Python applications. Further, our analysis of MicroPython projects
reveals the importance of hybrid analyses.",http://arxiv.org/abs/2109.01109v1,
An array-oriented Python interface for FastJet,Aryan Roy,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1088/1742-6596/2438/1/012011', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2202.03911v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2202.03911v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-02-08T14:57:31Z,"Analysis on HEP data is an iterative process in which the results of one step
often inform the next. In an exploratory analysis, it is common to perform one
computation on a collection of events, then view the results (often with
histograms) to decide what to try next. Awkward Array is a Scikit-HEP Python
package that enables data analysis with array-at-a-time operations to implement
cuts as slices, combinatorics as composable functions, etc. However, most C++
HEP libraries, such as FastJet, have an imperative, one-particle-at-a-time
interface, which would be inefficient in Python and goes against the grain of
the array-at-a-time logic of scientific Python. Therefore, we developed
fastjet, a pip-installable Python package that provides FastJet C++ binaries,
the classic (particle-at-a-time) Python interface, and the new array-oriented
interface for use with Awkward Array.
  The new interface streamlines interoperability with scientific Python
software beyond HEP, such as machine learning. In one case, adopting this
library along with other array-oriented tools accelerated HEP analysis code by
a factor of 20. It was designed to be easily integrated with libraries in the
Scikit-HEP ecosystem, including Uproot (file I/O), hist (histogramming), Vector
(Lorentz vectors), and Coffea (high-level glue). We discuss the design of the
fastjet Python library, integrating the classic interface with the array
oriented interface and with the Vector library for Lorentz vector operations.
The new interface was developed as open source.",http://arxiv.org/abs/2202.03911v1,
Deep Learning: From Basics to Building Deep Neural Networks with Python,Milad Vazan,"[{'@href': 'http://arxiv.org/abs/2205.01069v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2205.01069v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-04-22T11:57:19Z,"This book is intended for beginners who have no familiarity with deep
learning. Our only expectation from readers is that they already have the basic
programming skills in Python.",http://arxiv.org/abs/2205.01069v1,
The Awkward World of Python and C++,Manasvi Goyal,"[{'@href': 'http://arxiv.org/abs/2303.02205v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2303.02205v1', '@rel': 'related', '@type': 'application/pdf'}]",2023-03-03T20:33:50Z,"There are undeniable benefits of binding Python and C++ to take advantage of
the best features of both languages. This is especially relevant to the HEP and
other scientific communities that have invested heavily in the C++ frameworks
and are rapidly moving their data analyses to Python. Version 2 of Awkward
Array, a Scikit-HEP Python library, introduces a set of header-only C++
libraries that do not depend on any application binary interface. Users can
directly include these libraries in their compilation rather than linking
against platform-specific libraries. This new development makes the integration
of Awkward Arrays into other projects easier and more portable as the
implementation is easily separable from the rest of the Awkward Array codebase.
The code is minimal, it does not include all of the code needed to use Awkward
Arrays in Python, nor does it include references to Python or pybind11. The C++
users can use it to make arrays and then copy them to Python without any
specialized data types - only raw buffers, strings, and integers. This C++ code
also simplifies the process of just-in-time (JIT) compilation in ROOT. This
implementation approach solves some of the drawbacks, like packaging projects
where native dependencies can be challenging. In this paper, we demonstrate the
technique to integrate C++ and Python by using a header-only approach. We also
describe the implementation of a new LayoutBuilder and a GrowableBuffer.
Furthermore, examples of wrapping the C++ data into Awkward Arrays and exposing
Awkward Arrays to C++ without copying them are discussed.",http://arxiv.org/abs/2303.02205v1,
SlipCover: Near Zero-Overhead Code Coverage for Python,Juan Altmayer Pizzorno,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1145/3597926.3598128', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2305.02886v4', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.02886v4', '@rel': 'related', '@type': 'application/pdf'}]",2023-05-04T14:49:44Z,"Coverage analysis is widely used but can suffer from high overhead. This
overhead is especially acute in the context of Python, which is already
notoriously slow (a recent study observes a roughly 30x slowdown vs. native
code). We find that the state-of-the-art coverage tool for Python,
coverage$.$py, introduces a median overhead of 180% with the standard Python
interpreter. Slowdowns are even more extreme when using PyPy, a JIT-compiled
Python implementation, with coverage$.$py imposing a median overhead of 1,300%.
This performance degradation reduces the utility of coverage analysis in most
use cases, including testing and fuzzing, and precludes its use in deployment.
This paper presents SlipCover, a novel, near-zero overhead coverage analyzer
for Python. SlipCover works without modifications to either the Python
interpreter or PyPy. It first processes a program's AST to accurately identify
all branches and lines. SlipCover then dynamically rewrites Python bytecodes to
add lightweight instrumentation to each identified branch and line. At run
time, SlipCover periodically de-instruments already-covered lines and branches.
The result is extremely low overheads -- a median of just 5% -- making
SlipCover suitable for use in deployment. We show its efficiency can translate
to significant increases in the speed of coverage-based clients. As a proof of
concept, we integrate SlipCover into TPBT, a targeted property-based testing
system, and observe a 22x speedup.",http://arxiv.org/abs/2305.02886v4,
Cosmic Microwave Background Anisotropy Measurement From Python V,K. Coble,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1086/345714', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/astro-ph/0112506v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/astro-ph/0112506v2', '@rel': 'related', '@type': 'application/pdf'}]",2001-12-21T01:09:38Z,"We analyze observations of the microwave sky made with the Python experiment
in its fifth year of operation at the Amundsen-Scott South Pole Station in
Antarctica. After modeling the noise and constructing a map, we extract the
cosmic signal from the data. We simultaneously estimate the angular power
spectrum in eight bands ranging from large (l ~ 40) to small (l ~ 260) angular
scales, with power detected in the first six bands. There is a significant rise
in the power spectrum from large to smaller (l ~ 200) scales, consistent with
that expected from acoustic oscillations in the early Universe. We compare this
Python V map to a map made from data taken in the third year of Python. Python
III observations were made at a frequency of 90 GHz and covered a subset of the
region of the sky covered by Python V observations, which were made at 40 GHz.
Good agreement is obtained both visually (with a filtered version of the map)
and via a likelihood ratio test.",http://arxiv.org/abs/astro-ph/0112506v2,
"Solve the Master Equation by Python-An Introduction to the Python
  Computing Environment",Wei Fan,"[{'@href': 'http://arxiv.org/abs/1103.0325v4', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1103.0325v4', '@rel': 'related', '@type': 'application/pdf'}]",2011-03-02T00:46:20Z,"A brief introduction to the Python computing environment is given. By solving
the master equation encountered in quantum transport, we give an example of how
to solve the ODE problems in Python. The ODE solvers used are the ZVODE routine
in Scipy and the bsimp solver in GSL. For the former, the equation can be in
its complex-valued form, while for the latter, it has to be rewritten to a
real-valued form. The focus is on the detailed workflow of the implementation
process, rather than on the syntax of the python language, with the hope to
help readers simulate their own models in Python.",http://arxiv.org/abs/1103.0325v4,
"FluidFFT: common API (C++ and Python) for Fast Fourier Transform HPC
  libraries",Ashwin Vishnu Mohanan,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.5334/jors.238', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/1807.01775v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1807.01775v1', '@rel': 'related', '@type': 'application/pdf'}]",2018-07-03T09:52:57Z,"The Python package fluidfft provides a common Python API for performing Fast
Fourier Transforms (FFT) in sequential, in parallel and on GPU with different
FFT libraries (FFTW, P3DFFT, PFFT, cuFFT). fluidfft is a comprehensive FFT
framework which allows Python users to easily and efficiently perform FFT and
the associated tasks, such as as computing linear operators and energy spectra.
We describe the architecture of the package composed of C++ and Cython FFT
classes, Python ""operator"" classes and Pythran functions. The package supplies
utilities to easily test itself and benchmark the different FFT solutions for a
particular case and on a particular machine. We present a performance scaling
analysis on three different computing clusters and a microbenchmark showing
that fluidfft is an interesting solution to write efficient Python applications
using FFT.",http://arxiv.org/abs/1807.01775v1,
"Speeding simulation analysis up with yt and Intel Distribution for
  Python",Salvatore Cielo,"[{'@href': 'http://arxiv.org/abs/1910.07855v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1910.07855v1', '@rel': 'related', '@type': 'application/pdf'}]",2019-10-17T12:28:46Z,"As modern scientific simulations grow ever more in size and complexity, even
their analysis and post-processing becomes increasingly demanding, calling for
the use of HPC resources and methods. yt is a parallel, open source
post-processing python package for numerical simulations in astrophysics, made
popular by its cross-format compatibility, its active community of developers
and its integration with several other professional Python instruments. The
Intel Distribution for Python enhances yt's performance and parallel
scalability, through the optimization of lower-level libraries Numpy and Scipy,
which make use of the optimized Intel Math Kernel Library (Intel-MKL) and the
Intel MPI library for distributed computing. The library package yt is used for
several analysis tasks, including integration of derived quantities, volumetric
rendering, 2D phase plots, cosmological halo analysis and production of
synthetic X-ray observation. In this paper, we provide a brief tutorial for the
installation of yt and the Intel Distribution for Python, and the execution of
each analysis task. Compared to the Anaconda python distribution, using the
provided solution one can achieve net speedups up to 4.6x on Intel Xeon
Scalable processors (codename Skylake).",http://arxiv.org/abs/1910.07855v1,
Scalene: Scripting-Language Aware Profiling for Python,Emery D. Berger,"[{'@href': 'http://arxiv.org/abs/2006.03879v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2006.03879v2', '@rel': 'related', '@type': 'application/pdf'}]",2020-06-06T14:43:09Z,"Existing profilers for scripting languages (a.k.a. ""glue"" languages) like
Python suffer from numerous problems that drastically limit their usefulness.
They impose order-of-magnitude overheads, report information at too coarse a
granularity, or fail in the face of threads. Worse, past
profilers---essentially variants of their counterparts for C---are oblivious to
the fact that optimizing code in scripting languages requires information about
code spanning the divide between the scripting language and libraries written
in compiled languages.
  This paper introduces scripting-language aware profiling, and presents
Scalene, an implementation of scripting-language aware profiling for Python.
Scalene employs a combination of sampling, inference, and disassembly of
byte-codes to efficiently and precisely attribute execution time and memory
usage to either Python, which developers can optimize, or library code, which
they cannot. It includes a novel sampling memory allocator that reports
line-level memory consumption and trends with low overhead, helping developers
reduce footprints and identify leaks. Finally, it introduces a new metric, copy
volume, to help developers root out insidious copying costs across the
Python/library boundary, which can drastically degrade performance. Scalene
works for single or multi-threaded Python code, is precise, reporting detailed
information at the line granularity, while imposing modest overheads
(26%--53%).",http://arxiv.org/abs/2006.03879v2,
"Extending Python for Quantum-Classical Computing via Quantum
  Just-in-Time Compilation",Thien Nguyen,"[{'@href': 'http://arxiv.org/abs/2105.04671v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2105.04671v1', '@rel': 'related', '@type': 'application/pdf'}]",2021-05-10T21:11:21Z,"Python is a popular programming language known for its flexibility,
usability, readability, and focus on developer productivity. The quantum
software community has adopted Python on a number of large-scale efforts due to
these characteristics, as well as the remote nature of near-term quantum
processors. The use of Python has enabled quick prototyping for quantum code
that directly benefits pertinent research and development efforts in quantum
scientific computing. However, this rapid prototyping ability comes at the cost
of future performant integration for tightly-coupled CPU-QPU architectures with
fast-feedback. Here we present a language extension to Python that enables
heterogeneous quantum-classical computing via a robust C++ infrastructure for
quantum just-in-time (QJIT) compilation. Our work builds off the QCOR C++
language extension and compiler infrastructure to enable a single-source,
quantum hardware-agnostic approach to quantum-classical computing that retains
the performance required for tightly coupled CPU-QPU compute models. We detail
this Pythonic extension, its programming model and underlying software
architecture, and provide a robust set of examples to demonstrate the utility
of our approach.",http://arxiv.org/abs/2105.04671v1,
An Empirical Study of Automated Unit Test Generation for Python,Stephan Lukasczyk,"[{'@href': 'http://arxiv.org/abs/2111.05003v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2111.05003v2', '@rel': 'related', '@type': 'application/pdf'}]",2021-11-09T08:54:33Z,"Various mature automated test generation tools exist for statically typed
programming languages such as Java. Automatically generating unit tests for
dynamically typed programming languages such as Python, however, is
substantially more difficult due to the dynamic nature of these languages as
well as the lack of type information. Our Pynguin framework provides automated
unit test generation for Python. In this paper, we extend our previous work on
Pynguin to support more aspects of the Python language, and by studying a
larger variety of well-established state of the art test-generation algorithms,
namely DynaMOSA, MIO, and MOSA. Furthermore, we improved our Pynguin tool to
generate regression assertions, whose quality we also evaluate. Our experiments
confirm that evolutionary algorithms can outperform random test generation also
in the context of Python, and similar to the Java world, DynaMOSA yields the
highest coverage results. However, our results also demonstrate that there are
still fundamental remaining issues, such as inferring type information for code
without this information, currently limiting the effectiveness of test
generation for Python.",http://arxiv.org/abs/2111.05003v2,
"An Exploratory Study on the Predominant Programming Paradigms in Python
  Code",Robert Dyer,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1145/3540250.3549158', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2209.01817v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2209.01817v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-09-05T08:03:20Z,"Python is a multi-paradigm programming language that fully supports
object-oriented (OO) programming. The language allows writing code in a
non-procedural imperative manner, using procedures, using classes, or in a
functional style. To date, no one has studied what paradigm(s), if any, are
predominant in Python code and projects. In this work, we first define a
technique to classify Python files into predominant paradigm(s). We then
automate our approach and evaluate it against human judgements, showing over
80% agreement. We then analyze over 100k open-source Python projects,
automatically classifying each source file and investigating the paradigm
distributions. The results indicate Python developers tend to heavily favor OO
features. We also observed a positive correlation between OO and procedural
paradigms and the size of the project. And despite few files or projects being
predominantly functional, we still found many functional feature uses.",http://arxiv.org/abs/2209.01817v1,
A Data Set of Generalizable Python Code Change Patterns,Akalanka Galappaththi,"[{'@href': 'http://arxiv.org/abs/2304.04983v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2304.04983v1', '@rel': 'related', '@type': 'application/pdf'}]",2023-04-11T04:59:26Z,"Mining repetitive code changes from version control history is a common way
of discovering unknown change patterns. Such change patterns can be used in
code recommender systems or automated program repair techniques. While there
are such tools and datasets exist for Java, there is little work on finding and
recommending such changes in Python. In this paper, we present a data set of
manually vetted generalizable Python repetitive code change patterns. We create
a coding guideline to identify generalizable change patterns that can be used
in automated tooling. We leverage the mined change patterns from recent work
that mines repetitive changes in Python projects and use our coding guideline
to manually review the patterns. For each change, we also record a description
of the change and why it is applied along with other characteristics such as
the number of projects it occurs in. This review process allows us to identify
and share 72 Python change patterns that can be used to build and advance
Python developer support tools.",http://arxiv.org/abs/2304.04983v1,
Scalable Demand-Driven Call Graph Generation for Python,Yixuan Yan,"[{'@href': 'http://arxiv.org/abs/2305.05949v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.05949v1', '@rel': 'related', '@type': 'application/pdf'}]",2023-05-10T07:40:05Z,"Call graph generation is the foundation of inter-procedural static analysis.
PyCG is the state-of-the-art approach for generating call graphs for Python
programs. Unfortunately, PyCG does not scale to large programs when adapted to
whole-program analysis where dependent libraries are also analyzed. Further,
PyCG does not support demand-driven analysis where only the reachable functions
from given entry functions are analyzed. Moreover, PyCG is flow-insensitive and
does not fully support Python's features, hindering its accuracy. To overcome
these drawbacks, we propose a scalable demand-driven approach for generating
call graphs for Python programs, and implement it as a prototype tool Jarvis.
Jarvis maintains an assignment graph (i.e., points-to relations between program
identifiers) for each function in a program to allow reuse and improve
scalability. Given a set of entry functions as the demands, Jarvis generates
the call graph on-the-fly, where flow-sensitive intra-procedural analysis and
inter-procedural analysis are conducted in turn. Our evaluation on a
micro-benchmark of 135 small Python programs and a macro-benchmark of 6
real-world Python applications has demonstrated that Jarvis can significantly
improve PyCG by at least 67% faster in time, 84% higher in precision, and at
least 10% higher in recall.",http://arxiv.org/abs/2305.05949v1,
"Py-Tetrad and RPy-Tetrad: A New Python Interface with R Support for
  Tetrad Causal Search",Joseph D. Ramsey,"[{'@href': 'http://arxiv.org/abs/2308.07346v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2308.07346v1', '@rel': 'related', '@type': 'application/pdf'}]",2023-08-13T16:29:05Z,"We give novel Python and R interfaces for the (Java) Tetrad project for
causal modeling, search, and estimation. The Tetrad project is a mainstay in
the literature, having been under consistent development for over 30 years.
Some of its algorithms are now classics, like PC and FCI; others are recent
developments. It is increasingly the case, however, that researchers need to
access the underlying Java code from Python or R. Existing methods for doing
this are inadequate. We provide new, up-to-date methods using the JPype
Python-Java interface and the Reticulate Python-R interface, directly solving
these issues. With the addition of some simple tools and the provision of
working examples for both Python and R, using JPype and Reticulate to interface
Python and R with Tetrad is straightforward and intuitive.",http://arxiv.org/abs/2308.07346v1,
"High performance Python for direct numerical simulations of turbulent
  flows",Mikael Mortensen,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1016/j.cpc.2016.02.005', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/1602.03638v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1602.03638v1', '@rel': 'related', '@type': 'application/pdf'}]",2016-02-11T08:12:37Z,"Direct Numerical Simulations (DNS) of the Navier Stokes equations is an
invaluable research tool in fluid dynamics. Still, there are few publicly
available research codes and, due to the heavy number crunching implied,
available codes are usually written in low-level languages such as C/C++ or
Fortran. In this paper we describe a pure scientific Python pseudo-spectral DNS
code that nearly matches the performance of C++ for thousands of processors and
billions of unknowns. We also describe a version optimized through Cython, that
is found to match the speed of C++. The solvers are written from scratch in
Python, both the mesh, the MPI domain decomposition, and the temporal
integrators. The solvers have been verified and benchmarked on the Shaheen
supercomputer at the KAUST supercomputing laboratory, and we are able to show
very good scaling up to several thousand cores.
  A very important part of the implementation is the mesh decomposition (we
implement both slab and pencil decompositions) and 3D parallel Fast Fourier
Transforms (FFT). The mesh decomposition and FFT routines have been implemented
in Python using serial FFT routines (either NumPy, pyFFTW or any other serial
FFT module), NumPy array manipulations and with MPI communications handled by
MPI for Python (mpi4py). We show how we are able to execute a 3D parallel FFT
in Python for a slab mesh decomposition using 4 lines of compact Python code,
for which the parallel performance on Shaheen is found to be slightly better
than similar routines provided through the FFTW library. For a pencil mesh
decomposition 7 lines of code is required to execute a transform.",http://arxiv.org/abs/1602.03638v1,
PyCG: Practical Call Graph Generation in Python,Vitalis Salis,"[{'@href': 'http://arxiv.org/abs/2103.00587v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2103.00587v1', '@rel': 'related', '@type': 'application/pdf'}]",2021-02-28T18:49:25Z,"Call graphs play an important role in different contexts, such as profiling
and vulnerability propagation analysis. Generating call graphs in an efficient
manner can be a challenging task when it comes to high-level languages that are
modular and incorporate dynamic features and higher-order functions.
  Despite the language's popularity, there have been very few tools aiming to
generate call graphs for Python programs. Worse, these tools suffer from
several effectiveness issues that limit their practicality in realistic
programs. We propose a pragmatic, static approach for call graph generation in
Python. We compute all assignment relations between program identifiers of
functions, variables, classes, and modules through an inter-procedural
analysis. Based on these assignment relations, we produce the resulting call
graph by resolving all calls to potentially invoked functions. Notably, the
underlying analysis is designed to be efficient and scalable, handling several
Python features, such as modules, generators, function closures, and multiple
inheritance.
  We have evaluated our prototype implementation, which we call PyCG, using two
benchmarks: a micro-benchmark suite containing small Python programs and a set
of macro-benchmarks with several popular real-world Python packages. Our
results indicate that PyCG can efficiently handle thousands of lines of code in
less than a second (0.38 seconds for 1k LoC on average). Further, it
outperforms the state-of-the-art for Python in both precision and recall: PyCG
achieves high rates of precision ~99.2%, and adequate recall ~69.9%. Finally,
we demonstrate how PyCG can aid dependency impact analysis by showcasing a
potential enhancement to GitHub's ""security advisory"" notification service
using a real-world example.",http://arxiv.org/abs/2103.00587v1,
PyArmadillo: a streamlined linear algebra library for Python,Jason Rumengan,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.21105/joss.03051', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2104.11120v4', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2104.11120v4', '@rel': 'related', '@type': 'application/pdf'}]",2021-04-22T15:13:33Z,"PyArmadillo is a linear algebra library for the Python language, with the aim
of closely mirroring the programming interface of the widely used Armadillo C++
library, which in turn is deliberately similar to Matlab. PyArmadillo hence
facilitates algorithm prototyping with Matlab-like syntax directly in Python,
and relatively straightforward conversion of PyArmadillo-based Python code into
performant Armadillo-based C++ code. The converted code can be used for
purposes such as speeding up Python-based programs in conjunction with
pybind11, or the integration of algorithms originally prototyped in Python into
larger C++ codebases. PyArmadillo provides objects for matrices and cubes, as
well as over 200 associated functions for manipulating data stored in the
objects. Integer, floating point and complex numbers are supported. Various
matrix factorisations are provided through integration with LAPACK, or one of
its high performance drop-in replacements such as Intel MKL or OpenBLAS.
PyArmadillo is open-source software, distributed under the Apache 2.0 license;
it can be obtained at https://pyarma.sourceforge.io or via the Python Package
Index in precompiled form.",http://arxiv.org/abs/2104.11120v4,
"Python for Smarter Cities: Comparison of Python libraries for static and
  interactive visualisations of large vector data",Gregor Herda,"[{'@href': 'http://arxiv.org/abs/2202.13105v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2202.13105v1', '@rel': 'related', '@type': 'application/pdf'}]",2022-02-26T10:23:29Z,"Local governments, as part of 'smart city' initiatives and to promote
interoperability, are increasingly incorporating open-source software into
their data management, analysis, and visualisation workflows. Python, with its
concise and natural syntax, presents a low barrier to entry for municipal staff
without computer science backgrounds. However, with regard to geospatial
visualisations in particular, the range of available Python libraries has
diversified to such an extent that identifying candidate libraries for specific
use cases is a challenging undertaking. This study therefore assesses
prominent, actively-developed visualisation libraries in the Python ecosystem
with respect to their suitability for producing visualisations of large vector
datasets. A simple visualisation task common in urban development is used to
produce near-identical thematic maps across static and an interactive 'tracks'
of comparison. All short-listed libraries were able to generate the sample map
products for both a small and larger dataset. Code complexity differed more
strongly for interactive visualisations. Formal and informal documentation
channels are highlighted to outline available resources for flattening learning
curves. CPU runtimes for the Python-based portion of the process chain differed
starkly for both tracks, pointing to avenues for further research. These
results demonstrate that the Python ecosystem offers local governments powerful
tools, free of vendor lock-in and licensing fees, to produce performant and
consistently formatted visualisations for both internal and public
distribution.",http://arxiv.org/abs/2202.13105v1,
An Empirical Study of Fault Localization in Python Programs,Mohammad Rezaalipour,"[{'@href': 'http://arxiv.org/abs/2305.19834v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2305.19834v2', '@rel': 'related', '@type': 'application/pdf'}]",2023-05-31T13:21:30Z,"Despite its massive popularity as a programming language, especially in novel
domains like data science programs, there is comparatively little research
about fault localization that targets Python. Even though it is plausible that
several findings about programming languages like C/C++ and Java -- the most
common choices for fault localization research -- carry over to other
languages, whether the dynamic nature of Python and how the language is used in
practice affect the capabilities of classic fault localization approaches
remain open questions to investigate.
  This paper is the first large-scale empirical study of fault localization on
real-world Python programs and faults. Using Zou et al.'s recent large-scale
empirical study of fault localization in Java as the basis of our study, we
investigated the effectiveness (i.e., localization accuracy), efficiency (i.e.,
runtime performance), and other features (e.g., different entity granularities)
of seven well-known fault-localization techniques in four families
(spectrum-based, mutation-based, predicate switching, and stack-trace based) on
135 faults from 13 open-source Python projects from the BugsInPy curated
collection.
  The results replicate for Python several results known about Java, and shed
light on whether Python's peculiarities affect the capabilities of fault
localization. The replication package that accompanies this paper includes
detailed data about our experiments, as well as the tool FauxPy that we
implemented to conduct the study.",http://arxiv.org/abs/2305.19834v2,
"Does Python Smell Like Java? Tool Support for Design Defect Discovery in
  Python",Nicole Vavrová,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.22152/programming-journal.org/2017/1/11', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/1703.10882v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1703.10882v1', '@rel': 'related', '@type': 'application/pdf'}]",2017-03-31T12:47:50Z,"The context of this work is specification, detection and ultimately removal
of detectable harmful patterns in source code that are associated with defects
in design and implementation of software. In particular, we investigate five
code smells and four antipatterns previously defined in papers and books. Our
inquiry is about detecting those in source code written in Python programming
language, which is substantially different from all prior research, most of
which concerns Java or C-like languages. Our approach was that of software
engineers: we have processed existing research literature on the topic,
extracted both the abstract definitions of nine design defects and their
concrete implementation specifications, implemented them all in a tool we have
programmed and let it loose on a huge test set obtained from open source code
from thousands of GitHub projects. When it comes to knowledge, we have found
that more than twice as many methods in Python can be considered too long
(statistically extremely longer than their neighbours within the same project)
than in Java, but long parameter lists are seven times less likely to be found
in Python code than in Java code. We have also found that Functional
Decomposition, the way it was defined for Java, is not found in the Python code
at all, and Spaghetti Code and God Classes are extremely rare there as well.
The grounding and the confidence in these results comes from the fact that we
have performed our experiments on 32'058'823 lines of Python code, which is by
far the largest test set for a freely available Python parser. We have also
designed the experiment in such a way that it aligned with prior research on
design defect detection in Java in order to ease the comparison if we treat our
own actions as a replication. Thus, the importance of the work is both in the
unique open Python grammar of highest quality, tested on millions of lines of
code, and in the design defect detection tool which works on something else
than Java.",http://arxiv.org/abs/1703.10882v1,
"Python I, II, and III CMB Anisotropy Measurement Constraints on Open and
  Flat-Lambda CDM Cosmogonies",Graca Rocha,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1086/307886', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/astro-ph/9905127v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/astro-ph/9905127v1', '@rel': 'related', '@type': 'application/pdf'}]",1999-05-11T18:16:35Z,"We use Python I, II, and III cosmic microwave background anisotropy data to
constrain cosmogonies. We account for the Python beamwidth and calibration
uncertainties. We consider open and spatially-flat-Lambda cold dark matter
cosmogonies, with nonrelativistic-mass density parameter Omega_0 in the range
0.1--1, baryonic-mass density parameter Omega_B in the range (0.005--0.029)
h^{-2}, and age of the universe t_0 in the range (10--20) Gyr. Marginalizing
over all parameters but Omega_0, the combined Python data favors an open
(spatially-flat-Lambda) model with Omega_0 simeq 0.2 (0.1). At the 2 sigma
confidence level model normalizations deduced from the combined Python data are
mostly consistent with those drawn from the DMR, UCSB South Pole 1994, ARGO,
MAX 4 and 5, White Dish, and SuZIE data sets.",http://arxiv.org/abs/astro-ph/9905127v1,
A Python-based Post-processing Toolset For Seismic Analyses,Steve Brasier,"[{'@href': 'http://arxiv.org/abs/1412.6410v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1412.6410v1', '@rel': 'related', '@type': 'application/pdf'}]",2014-12-19T16:09:16Z,"This paper discusses the design and implementation of a Python-based toolset
to aid in assessing the response of the UK's Advanced Gas Reactor nuclear power
stations to earthquakes. The seismic analyses themselves are carried out with a
commercial Finite Element solver, but understanding the raw model output this
produces requires customised post-processing and visualisation tools. Extending
the existing tools had become increasingly difficult and a decision was made to
develop a new, Python-based toolset. This comprises of a post-processing
framework (aftershock) which includes an embedded Python interpreter, and a
plotting package (afterplot) based on numpy and matplotlib. The new toolset had
to be significantly more flexible and easier to maintain than the existing
code-base, while allowing the majority of development to be carried out by
engineers with little training in software development. The resulting
architecture will be described with a focus on exploring how the design drivers
were met and the successes and challenges arising from the choices made.",http://arxiv.org/abs/1412.6410v1,
"Rabacus: A Python Package for Analytic Cosmological Radiative Transfer
  Calculations",Gabriel Altay,"[{'@title': 'doi', '@href': 'http://dx.doi.org/10.1016/j.ascom.2015.01.004', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/1502.02798v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1502.02798v2', '@rel': 'related', '@type': 'application/pdf'}]",2015-02-10T07:03:42Z,"We describe Rabacus, a Python package for calculating the transfer of
hydrogen ionizing radiation in simplified geometries relevant to astronomy and
cosmology. We present example solutions for three specific cases: 1) a
semi-infinite slab gas distribution in a homogeneous isotropic background, 2) a
spherically symmetric gas distribution with a point source at the center, and
3) a spherically symmetric gas distribution in a homogeneous isotropic
background. All problems can accommodate arbitrary spectra and density profiles
as input. The solutions include a treatment of both hydrogen and helium, a
self-consistent calculation of equilibrium temperatures, and the transfer of
recombination radiation. The core routines are written in Fortran 90 and then
wrapped in Python leading to execution speeds thousands of times faster than
equivalent routines written in pure Python. In addition, all variables have
associated units for ease of analysis. The software is part of the Python
Package Index and the source code is available on Bitbucket at
https://bitbucket.org/galtay/rabacus . In addition, installation instructions
and a detailed users guide are available at http://pythonhosted.org//rabacus .",http://arxiv.org/abs/1502.02798v2,
Weighted graph algorithms with Python,A. Kapanowski,"[{'@href': 'http://arxiv.org/abs/1504.07828v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1504.07828v1', '@rel': 'related', '@type': 'application/pdf'}]",2015-04-29T12:20:20Z,"Python implementation of selected weighted graph algorithms is presented. The
minimal graph interface is defined together with several classes implementing
this interface. Graph nodes can be any hashable Python objects. Directed edges
are instances of the Edge class. Graphs are instances of the Graph class. It is
based on the adjacency-list representation, but with fast lookup of nodes and
neighbors (dict-of-dict structure). Other implementations of this class are
also possible.
  In this work, many algorithms are implemented using a unified approach. There
are separate classes and modules devoted to different algorithms. Three
algorithms for finding a minimum spanning tree are implemented: the Boruvka's
algorithm, the Prim's algorithm (three implementations), and the Kruskal's
algorithm. Three algorithms for solving the single-source shortest path problem
are implemented: the dag shortest path algorithm, the Bellman-Ford algorithm,
and the Dijkstra's algorithm (two implementations). Two algorithms for solving
all-pairs shortest path problem are implemented: the Floyd-Warshall algorithm
and the Johnson's algorithm.
  All algorithms were tested by means of the unittest module, the Python unit
testing framework. Additional computer experiments were done in order to
compare real and theoretical computational complexity. The source code is
available from the public GitHub repository.",http://arxiv.org/abs/1504.07828v1,
"Garbage Collection in JyNI - How to bridge Mark/Sweep and Reference
  Counting GC",Stefan Richthofer,"[{'@href': 'http://arxiv.org/abs/1607.00825v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/1607.00825v1', '@rel': 'related', '@type': 'application/pdf'}]",2016-07-01T19:11:01Z,"Jython is a Java-based Python implementation and the most seamless way to
integrate Python and Java. It achieves high efficiency by compiling Python code
to Java bytecode and thus letting Java's JIT optimize it - an approach that
enables Python code to call Java functions or to subclass Java classes. It
enables Python code to leverage Java's multithreading features and utilizes
Java's built-in garbage collection (GC). However, it currently does not support
CPython's C-API and thus does not support native extensions like NumPy and
SciPy. Since most scientific code depends on such extensions, it is not
runnable with Jython. Jython Native Interface (JyNI) is a compatibility layer
that aims to provide CPython's native C extension API on top of Jython. JyNI is
implemented using the Java Native Interface (JNI) and its native part is
designed to be binary compatible with existing extension builds [...].",http://arxiv.org/abs/1607.00825v1,
